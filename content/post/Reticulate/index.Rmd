---
title: "The Beauty of Reticulate"
author: "Ryan Bailey"
date: '2020-12-08'


thumbnail: "featured.jpg"
image:
  caption: 'Photo by Romina FarÃ­as  on Unsplash'
  placement: 3
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
---

Python and R are integral to data science, period. There is no way around using them. Each language is great in its own right, but together, they're unstoppable.  In the below notebook, I am training a scikit-learn Support Vector Machine (SVM) classifier on [Diabetic Retinopathy Debrecen Dataset](https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set). If you're unfamiliar with SVMs, I highly recommend [this article by Rohith Gandhi](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47) from the Towards Data Science blog on Medium. After we build the classifier, I do some hyperparameter tuning until I reach a reasonable accuracy. 

All this stuff can be done in Python easily. But I want to use ggplot to visualize the dataset. Reticulate lets R and Python communicate with each others' environment. After I finish the classifier, I reduced the dimensionality of the dataset via UMAP. At the very bottom of the notebook, I show how the SVM's `predict` functionality is preserved from Python to R.  


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load in the Reticulate library
```{r}
library(reticulate)
```

## Check the Python configuration to make sure the paths and versions look good
```{r}
py_config()
```

## Importing Packages
```{python}
#!/usr/bin/env python
# coding: utf-8

#You may add additional imports
import warnings
warnings.simplefilter("ignore")
import pandas as pd
import numpy as np
import sklearn as sk
import matplotlib.pyplot as plt
import time
from sklearn.model_selection import GridSearchCV as GSCV
from sklearn.model_selection import cross_val_score
```

##Load in the dataset (Diabetic Retinopathy Debrecen Data Set)
```{python}
col_names = []
for i in range(20):
    if i == 0:
        col_names.append('quality')
    if i == 1:
        col_names.append('prescreen')
    if i >= 2 and i <= 7:
        col_names.append('ma' + str(i))
    if i >= 8 and i <= 15:
        col_names.append('exudate' + str(i))
    if i == 16:
        col_names.append('euDist')
    if i == 17:
        col_names.append('diameter')
    if i == 18:
        col_names.append('amfm_class')
    if i == 19:
        col_names.append('label')

data = pd.read_csv("messidor_features.txt", names = col_names)
print(data.shape)
data.head(10)
```

```{python}
datay = data['label']
datax = data.drop('label',axis =1 )
print(datay.shape)
print(datax.shape)
datax.head()
```

## Building a pipeline to first scale the data, then to train the model
```{python}
from sklearn.preprocessing import StandardScaler as SS
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
SS = SS()
clf = SVC()
pipe = Pipeline(steps=[('scaler', SS), ('SVC', clf)])

nested_score = cross_val_score(pipe, datax, datay, cv=5)
print('Mean Nested Score:',nested_score.mean())

```

## Hyperparameter Tuning
```{python}
# for the 'svm' part of the pipeline, tune the 'kernel' hyperparameter
param_grid = {'SVC__kernel': ['linear', 'rbf', 'poly', 'sigmoid']}
grid_search = GSCV(pipe, param_grid, cv=5, scoring='accuracy')
grid_search.fit(datax,datay)
best_kernel = grid_search.best_params_.get('SVC__kernel')
print(grid_search.best_params_)
print("Accuracy:", grid_search.best_score_)
```

## Nested cross validation mean accuracy
```{python}
nested_score = cross_val_score(grid_search, datax, datay, cv=5)
print('Mean Nested Score: ',nested_score.mean())
```

## Cross Validating the Hyperparameter-tuned Model
```{python}
param_grid = {'SVC__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],'SVC__C':list(range(50,110,10))}
grid_search = GSCV(pipe, param_grid, cv=5, scoring='accuracy')
grid_search.fit(datax,datay)
best_kernel = grid_search.best_params_.get('SVC__kernel')
print(grid_search.best_params_)
print("Accuracy:", grid_search.best_score_)

nested_score = cross_val_score(grid_search, datax, datay, cv=5)
print('Mean Nested Score:',nested_score.mean())



```

## Loading in R packages
```{r}
library(umap)
library(readr)
library(tidyverse)
library(ggplot2)
library(ggalt)
library(ggforce)
library(concaveman)
```

# Collapsing the data with UMAP
```{r}
set.seed(3)
labels <- py$data %>% pull(label)
data <- py$data %>% select(-label)
data_umap = umap(data)
umap_features <- cbind(((data_umap$layout) %>% as.data.frame),labels) %>% 
  mutate(label = as.factor(labels)) %>% 
  select(-labels)
umap_features %>% head
```

#### Plot the UMAP Data
```{r}
umap_features %>% 
  ggplot(aes(x = V1, y =V2, color = as.factor(labels))) + 
  geom_point(size = 3) + theme_minimal() + 
  geom_mark_ellipse(expand = 0,aes(fill=as.factor(labels))) +
  xlab("UMAP Feature 1") + 
  ylab("UMAP Feature 2")
```

## Extracting the predict method from the SVM
```{r}
predict <- py$grid_search$predict
set.seed(1)
predict(data[sample(1:nrow(data),2),])
```

