[{"authors":["admin"],"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ryan-bailey/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ryan-bailey/","section":"authors","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.","tags":null,"title":"Ryan Bailey","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" library(reticulate) py_config() ## python: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate/bin/python ## libpython: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate/lib/libpython3.6m.dylib ## pythonhome: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate:/Users/ryanbailey/Library/r-miniconda/envs/r-reticulate ## version: 3.6.11 | packaged by conda-forge | (default, Aug 5 2020, 20:19:23) [GCC Clang 10.0.1 ] ## numpy: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/numpy ## numpy_version: 1.19.4 #!/usr/bin/env python # coding: utf-8 #You may add additional imports import warnings warnings.simplefilter(\u0026quot;ignore\u0026quot;) import pandas as pd import numpy as np import sklearn as sk import matplotlib.pyplot as plt import time from sklearn.model_selection import GridSearchCV as GSCV from sklearn.model_selection import cross_val_score  # In[3]: # In[4]: # Read the data from csv file col_names = [] for i in range(20): if i == 0: col_names.append(\u0026#39;quality\u0026#39;) if i == 1: col_names.append(\u0026#39;prescreen\u0026#39;) if i \u0026gt;= 2 and i \u0026lt;= 7: col_names.append(\u0026#39;ma\u0026#39; + str(i)) if i \u0026gt;= 8 and i \u0026lt;= 15: col_names.append(\u0026#39;exudate\u0026#39; + str(i)) if i == 16: col_names.append(\u0026#39;euDist\u0026#39;) if i == 17: col_names.append(\u0026#39;diameter\u0026#39;) if i == 18: col_names.append(\u0026#39;amfm_class\u0026#39;) if i == 19: col_names.append(\u0026#39;label\u0026#39;) data = pd.read_csv(\u0026quot;messidor_features.txt\u0026quot;, names = col_names) print(data.shape) ## (1151, 20) data.head(10) # ### 1. Data prep # Q1. Separate the feature columns from the class label column. You should end up with two separate data frames - one that contains all of the feature values and one that contains the class labels. Print the shape of the features DataFrame, the shape of the labels DataFrame, and the head of the features DataFrame. # In[5]: # your code goes here ## quality prescreen ma2 ma3 ... euDist diameter amfm_class label ## 0 1 1 22 22 ... 0.486903 0.100025 1 0 ## 1 1 1 24 24 ... 0.520908 0.144414 0 0 ## 2 1 1 62 60 ... 0.530904 0.128548 0 1 ## 3 1 1 55 53 ... 0.483284 0.114790 0 0 ## 4 1 1 44 44 ... 0.475935 0.123572 0 1 ## 5 1 1 44 43 ... 0.502831 0.126741 0 1 ## 6 1 0 29 29 ... 0.541743 0.139575 0 1 ## 7 1 1 6 6 ... 0.576318 0.071071 1 0 ## 8 1 1 22 21 ... 0.500073 0.116793 0 1 ## 9 1 1 79 75 ... 0.560959 0.109134 0 1 ## ## [10 rows x 20 columns] datay = data[\u0026#39;label\u0026#39;] datax = data.drop(\u0026#39;label\u0026#39;,axis =1 ) print(datay.shape) ## (1151,) print(datax.shape) ## (1151, 19) datax.head() # ### 2. Support Vector Machines (SVM) and Pipelines # Q2. For some classification algorithms, like KNN, SVMs, and Neural Nets, scaling of the data is critical for the algorithm to operate correctly. For other classification algorithms, like Naive Bayes, and Decision Trees, data scaling is not necessary (take a minute to think about why that is the case). # # We discussed in class how the data scaling should happen on the _training set only_, which means that it should happen _inside_ of the cross validation loop. In other words, in each fold of the cross validation, the data will be separated in to training and test sets. The scaling (calculating mean and std, for instance) should happen based on the values in the _traning set only_. Then the test set can be scaled using the values found on the training set. (Refer to the concept of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).) # # In order to do this with scikit-learn, you must create what\u0026#39;s called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let\u0026#39;s practice it here. # # Do the following: # * Create a `sklearn.preprocessing.StandardScaler` object to standardize the dataset’s features (mean = 0 and variance = 1). Do not call `fit` on it yet. Just create the `StandardScaler` object. # * Create a sklearn.svm.SVC classifier (do not set any arguments - use the defaults). Do not call fit on it yet. Just create the SVC object. # * Create a `sklearn.pipeline.Pipeline` and set the `steps` to the scaler and the SVC objects that you just created. # * Pass the `pipeline` in to a `cross_val_score` as the estimator, along with the features and the labels, and use a 5-fold-CV. # # In each fold of the cross validation, the training phase will use _only_ the training data for scaling and training the model. Then the testing phase will scale the test data into the scaled space (found on the training data) and run the test data through the trained classifier, to return an accuracy measurement for each fold. Print the average accuracy across all 5 folds. # In[6]: # your code goes here ## quality prescreen ma2 ma3 ... exudate15 euDist diameter amfm_class ## 0 1 1 22 22 ... 0.003923 0.486903 0.100025 1 ## 1 1 1 24 24 ... 0.003903 0.520908 0.144414 0 ## 2 1 1 62 60 ... 0.007744 0.530904 0.128548 0 ## 3 1 1 55 53 ... 0.001531 0.483284 0.114790 0 ## 4 1 1 44 44 ... 0.000000 0.475935 0.123572 0 ## ## [5 rows x 19 columns] from sklearn.preprocessing import StandardScaler as SS from sklearn.svm import SVC from sklearn.pipeline import Pipeline SS = SS() clf = SVC() pipe = Pipeline(steps=[(\u0026#39;scaler\u0026#39;, SS), (\u0026#39;SVC\u0026#39;, clf)]) nested_score = cross_val_score(pipe, datax, datay, cv=5) print(\u0026#39;Mean Nested Score:\u0026#39;,nested_score.mean()) # Q3. The `svm.SVC` defaults to using an rbf (radial basis function) kernel. This kernel may or may not be the best choice for our dataset. We can use nested cross validation to find the best kernel for this dataset. # # Set up the inner CV loop: # * Starter code is provided to create the \u0026quot;parameter grid\u0026quot; to search. You will need to change this code! Where I have \u0026quot;svm__kernel\u0026quot;, this indicates that I want to tune the \u0026quot;kernel\u0026quot; parameter in the \u0026quot;svm\u0026quot; part of the pipeline. When you created your pipeline above, you named the SVM part of the pipeline with a string. You should replace \u0026quot;svm\u0026quot; in the param_grid below with whatever you named your SVM part of the pipeline: **\u0026lt;replace_this\u0026gt;__kernel.** # * Create a `sklearn.model_selection.GridSearchCV` that takes in the pipeline you created above (as the estimator), the parameter grid, and uses a 5-fold-CV. Call `fit` on the `GridSearchCV` to find the best kernel. # * Print out the best kernel (`best_params_`) for this dataset. # In[7]: # for the \u0026#39;svm\u0026#39; part of the pipeline, tune the \u0026#39;kernel\u0026#39; hyperparameter ## Mean Nested Score: 0.7011368341803125 param_grid = {\u0026#39;SVC__kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;, \u0026#39;poly\u0026#39;, \u0026#39;sigmoid\u0026#39;]} grid_search = GSCV(pipe, param_grid, cv=5, scoring=\u0026#39;accuracy\u0026#39;) grid_search.fit(datax,datay) ## GridSearchCV(cv=5, ## estimator=Pipeline(steps=[(\u0026#39;scaler\u0026#39;, StandardScaler()), ## (\u0026#39;SVC\u0026#39;, SVC())]), ## param_grid={\u0026#39;SVC__kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;, \u0026#39;poly\u0026#39;, \u0026#39;sigmoid\u0026#39;]}, ## scoring=\u0026#39;accuracy\u0026#39;) best_kernel = grid_search.best_params_.get(\u0026#39;SVC__kernel\u0026#39;) print(grid_search.best_params_) ## {\u0026#39;SVC__kernel\u0026#39;: \u0026#39;linear\u0026#39;} print(\u0026quot;Accuracy:\u0026quot;, grid_search.best_score_) # your code goes here # Q4. Now put what you did in Q3 in to an outer CV loop to evaluate the accuracy of using that best-found kernel on unseen test data. # * Pass the `GridSearchCV` in to a `cross_val_score` with 5-fold-CV. Print out the accuracy. # # Note that the accuracy increases from Q2 because of a better choice of kernel function. # In[8]: # your code goes here  ## Accuracy: 0.7228646715603239 nested_score = cross_val_score(grid_search, datax, datay, cv=5) print(\u0026#39;Mean Nested Score: \u0026#39;,nested_score.mean()) # Q5. Let\u0026#39;s see if we can get the accuracy even higher by tuning additional hyperparameters. SVMs have a parameter called \u0026#39;C\u0026#39; that is the cost for a misclassification. (More info [here](https://medium.com/@pushkarmandot/what-is-the-significance-of-c-value-in-support-vector-machine-28224e852c5a)). # * Create a parameter grid that includes the kernel (as you have above) and the C value as well. Try values of C from 50 to 100 by increments of 10. (You can use the range function to help you with this.) # * Create a `GridSearchCV` with the pipeline from above, this new parameter grid, and a 5-fold-CV. # * Pass the `GridSearchCV` into a `cross_val_score` with a 5-fold-CV and print out the accuracy. # # Be patient as this can take some time to run. Note that the accurcay has increased even further because the best value of C was found and used on the test data. # # Now we\u0026#39;re actually starting to get closer to some decent accuracies on this dataset! # In[9]: # your code goes here ## Mean Nested Score: 0.7228646715603239 param_grid = {\u0026#39;SVC__kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;, \u0026#39;poly\u0026#39;, \u0026#39;sigmoid\u0026#39;],\u0026#39;SVC__C\u0026#39;:list(range(50,110,10))} grid_search = GSCV(pipe, param_grid, cv=5, scoring=\u0026#39;accuracy\u0026#39;) grid_search.fit(datax,datay) ## GridSearchCV(cv=5, ## estimator=Pipeline(steps=[(\u0026#39;scaler\u0026#39;, StandardScaler()), ## (\u0026#39;SVC\u0026#39;, SVC())]), ## param_grid={\u0026#39;SVC__C\u0026#39;: [50, 60, 70, 80, 90, 100], ## \u0026#39;SVC__kernel\u0026#39;: [\u0026#39;linear\u0026#39;, \u0026#39;rbf\u0026#39;, \u0026#39;poly\u0026#39;, \u0026#39;sigmoid\u0026#39;]}, ## scoring=\u0026#39;accuracy\u0026#39;) best_kernel = grid_search.best_params_.get(\u0026#39;SVC__kernel\u0026#39;) print(grid_search.best_params_) ## {\u0026#39;SVC__C\u0026#39;: 70, \u0026#39;SVC__kernel\u0026#39;: \u0026#39;linear\u0026#39;} print(\u0026quot;Accuracy:\u0026quot;, grid_search.best_score_) ## Accuracy: 0.7463052889139845 nested_score = cross_val_score(grid_search, datax, datay, cv=5) print(\u0026#39;Mean Nested Score:\u0026#39;,nested_score.mean()) # ## Mean Nested Score: 0.7454357236965933 ","date":1607385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607385600,"objectID":"d5bc0e3899ee8e3bc4e72f114920a99f","permalink":"/post/reticulate/","publishdate":"2020-12-08T00:00:00Z","relpermalink":"/post/reticulate/","section":"post","summary":"library(reticulate) py_config() ## python: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate/bin/python ## libpython: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate/lib/libpython3.6m.dylib ## pythonhome: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate:/Users/ryanbailey/Library/r-miniconda/envs/r-reticulate ## version: 3.6.11 | packaged by conda-forge | (default, Aug 5 2020, 20:19:23) [GCC Clang 10.0.1 ] ## numpy: /Users/ryanbailey/Library/r-miniconda/envs/r-reticulate/lib/python3.","tags":null,"title":"The Beauty of Reticulate","type":"post"},{"authors":null,"categories":null,"content":" REB3566  Welcome to for loop h*ll  0a. Introduction This analysis is based on The Cancer Genome Atlas’ Pancreatic Adenocarcinoma Project (TCGA-PAAD). TCGA provides large, well-documented cancer datasets that are semi open-source. Three datasets, HT-Seq FPKM, survival data, and phenotype data, were acquired from UCSC Xena, a data download portal from UC Santa Cruz UCSC Xena. There are 182 complete cases for these datasets. The HT-Seq dataset has 60,483 variables, all of which are numeric. These are measured in log2(fpkm+1) or log2(Fragments Per Kilobase of transcript per Million mapped reads + 1). In RNA-Seq, the relative expression of a transcript is proportional to the number of cDNA fragments that originate from it. The survival data contains two variables, days_survived (OS.time) and OS (Overall Survival), where 1 is a survival event (death), and 0 is no survival event. The phenotype data contains 122 variables encompassing treatment information, tumor grading/staging, patient health behaviors, and demographic information.\nOne of the key categorical variables that I come back to frequently is organ of origin. When I binarize this variable, I encode Head of Pancreas as 1 and all other locations as 0. For context, pancreatic cancers that originate in the head of the pancreas often have better prognoses than those that arise in other areas.\nNote: I plan on doing inline comments on my code, but will also narrate my results, interpretations, and thought processes after each code chunk.\n 0b. Importing Libraries # for reading in my datasets library(readr) # for data wrangling, cleaning, etc. library(tidyverse) # plotting library(ggplot2) # Heteroscedasticity-Consistent Covariance Matrix # Estimation library(sandwich) # test for heteroskedasticity library(lmtest) # ROC plots library(plotROC) # LASSO library(glmnet) # Things to build a hierarchically clustered # heatmap library(prettyR) library(reshape2) library(ComplexHeatmap) library(circlize) # for PCA Biplot library(factoextra) # for plotting interactions library(interactions) # manova assumptions library(rstatix)  0c-g: Github didn’t like my 130 Mb dataset so I just created a .csv with my EDAed/subset data and import that data \u0026lt;- read_csv(\u0026quot;Project2_data.csv\u0026quot;)  0h. PCA set.seed(1) data_pca \u0026lt;- data %\u0026gt;% select(contains(\u0026quot;ENSG\u0026quot;)) %\u0026gt;% scale %\u0026gt;% princomp data_pca_df \u0026lt;- data.frame(PC1 = data_pca$scores[, 1], PC2 = data_pca$scores[, 2], PC3 = data_pca$scores[, 3], HoP = as.factor(data$Head_of_pancreas)) ggplot(data_pca_df, aes(PC1, PC2, color = HoP)) + geom_point() + theme_minimal() + geom_point(data = data_pca_df %\u0026gt;% group_by(HoP) %\u0026gt;% summarize(x = mean(PC2), y = mean(PC2)), aes(x, y), size = 6, shape = 3) + ggtitle(\u0026quot;PCA of Data Colored By Head of Pancreas\u0026quot;) It became instantly obvious to me that MANOVA and all of the hypothesis tests for its assumptions break down with high dimensional data. I’m not sure why this is. Rather than hand picking or randomly picking a subset of variables, I perform PCA on the data and work with principal components instead.\n 1a. Assessing MANOVA Assumptions group \u0026lt;- as.character(data_pca_df$HoP) DVs \u0026lt;- data_pca_df %\u0026gt;% select(PC1, PC2, PC3) # Test multivariate normality for each group (null: # assumption met) sapply(split(DVs, group), mshapiro_test) ## 0 1 ## statistic 0.9147001 0.9350529 ## p.value 0.001200223 9.521684e-06 # If any p\u0026lt;.05, stop (assumption violated). If not, # test homogeneity of covariance matrices # Box\u0026#39;s M test (null: assumption met) # box_m(DVs, group) Welp, it appears I violate some of the assumptions of the MANOVA, but that’s no surprise. The samples are not likely to be fully random. PCA1, PC2, and PC3 are neither univariately normal nor multivariately normal. There is not a strong linear relationship between PC1, PC2, PC3 There are several groups of outliers for the variables.\nBox’s test was not performed because the data did not meet the multivariate normality assumption.\n 1b. MANOVA: Head of Pancreas and Principal Components man \u0026lt;- manova(cbind(PC1, PC2, PC3) ~ HoP, data = data_pca_df) summary(man, tol = 0) ## Df Pillai approx F num Df den Df Pr(\u0026gt;F) ## HoP 1 0.10262 6.7852 3 178 0.000234 *** ## Residuals 180 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 summary.aov(man) ## Response PC1 : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## HoP 1 209.8 209.843 8.114 0.004904 ** ## Residuals 180 4655.1 25.862 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response PC2 : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## HoP 1 41.66 41.659 5.5778 0.01926 * ## Residuals 180 1344.37 7.469 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Response PC3 : ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## HoP 1 24.24 24.2394 5.4584 0.02058 * ## Residuals 180 799.34 4.4408 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 pairwise.t.test((data_pca_df$PC1), data_pca_df$HoP, p.adj = \u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: (data_pca_df$PC1) and data_pca_df$HoP ## ## 0 ## 1 0.0049 ## ## P value adjustment method: none pairwise.t.test((data_pca_df$PC2), data_pca_df$HoP, p.adj = \u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: (data_pca_df$PC2) and data_pca_df$HoP ## ## 0 ## 1 0.019 ## ## P value adjustment method: none pairwise.t.test((data_pca_df$PC3), data_pca_df$HoP, p.adj = \u0026quot;none\u0026quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: (data_pca_df$PC3) and data_pca_df$HoP ## ## 0 ## 1 0.021 ## ## P value adjustment method: none bonf_p \u0026lt;- 0.05/7 bonf_p ## [1] 0.007142857 prob_typeI \u0026lt;- 1 - (0.95^7) prob_typeI ## [1] 0.3016627 Significant differences were found between head of pancreas samples and non-head of pancreas samples. After performing 7 hypothesis tests, the probablility of a Type I error was 0.3016627. The bonferroni-corrected p-value is 0.007142857. While all hypothesis tests returned p-values less than 0.05. Only the MANOVA, PC1 ANOVA, and the PC1 pairwise t-test returned p-values less than the bonf_p. This means that only head of pancreas and non-head of pancreas only significantly different for principal component 1.\n 2a. Randomization Tests: Mean Difference in PC Values in HoP = 1 vs HoP = 0 # set the seed set.seed(1) # randomization_test takes a data.frame, a column # name (\u0026#39;string\u0026#39;), and an optional iters argument # it returns a histogram of bootstrapped mean # distances. randomization_test \u0026lt;- function(data, column, iters = 5000) { rand_dist \u0026lt;- c() new \u0026lt;- data %\u0026gt;% select(column, HoP) for (i in 1:iters) { temp \u0026lt;- new[sample(1:nrow(new), size = 10, replace = T), ] %\u0026gt;% group_by(HoP) %\u0026gt;% summarize_if(is.numeric, mean) if (nrow(temp) == 1) { rand_dist[i] \u0026lt;- temp %\u0026gt;% pull(column) } if (nrow(temp) == 2) { rand_dist[i] \u0026lt;- temp %\u0026gt;% summarize_if(is.numeric, diff) %\u0026gt;% pull(column) } } # get the lower and upper bounds of all the # mean_dists for that gene lb \u0026lt;- quantile(rand_dist, 0.025) ub \u0026lt;- quantile(rand_dist, 0.975) data.frame(means = rand_dist) %\u0026gt;% ggplot(aes(x = means)) + geom_histogram(alpha = 0.75, fill = \u0026quot;gray\u0026quot;) + theme_minimal() + geom_vline(xintercept = mean(rand_dist), color = \u0026quot;red\u0026quot;) + geom_vline(xintercept = lb, color = \u0026quot;green\u0026quot;) + geom_vline(xintercept = ub, color = \u0026quot;green\u0026quot;) + ggtitle(paste(i, \u0026quot;Bootstrapped Mean Differences Between HoP = 1 vs HoP = 0 for\u0026quot;, column)) + xlab(\u0026quot;Differences in Means\u0026quot;) } The null hypothesis for each of the three hypothesis tests is that the HoP = 1 group and HoP = 0 group do not differ in their means.\nThe alternative hypothesis for each of the three hypothesis tests is that the HoP = 1 group and HoP = 0 group do differ in their means.\nFor context, the red line in each histogram is the mean, and the green lines are the 95 CI. It’s pretty clear to me that in each of the three plots, the 95 CI encompasses 0.\nThe mean distributions between Head of pancreas (HoP = 1) and non-Head of pancreas (HoP = 0) samples was not significantly different for PC1, PC2, or PC3. This is because for each variable, the 95 CIs of mean differences encompass 0. I cannot reject the null hypothesis for any of these variables.\n 3a. LM: Predicting Survival Time from Principal Components # create mean centered principal component # variables; rename OS.time to `y` data_pca_df \u0026lt;- data_pca_df %\u0026gt;% mutate(PC1_c = PC1 - mean(PC1), PC2_c = PC2 - mean(PC2)) %\u0026gt;% mutate(y = data$OS.time) fit \u0026lt;- lm(y ~ PC1_c * PC2_c, data = data_pca_df) # check normality qqnorm(fit$residuals, main = \u0026quot;QQ-plot of Model Residuals\u0026quot;) qqline(fit$residuals, col = \u0026quot;red\u0026quot;) # check for uniform variance; check linearity res \u0026lt;- data.frame(fitted.values = fit$fitted.values, residuals = fit$residuals) res %\u0026gt;% ggplot(aes(fitted.values, residuals)) + geom_smooth(color = \u0026quot;black\u0026quot;, se = T) + geom_point() + geom_hline(aes(yintercept = 0), color = \u0026quot;red\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;Plotting Residuals by Fitted Values\u0026quot;) # hypothesis test for uniform variance bptest(fit) ## ## studentized Breusch-Pagan test ## ## data: fit ## BP = 4.4287, df = 3, p-value = 0.2187 # model summary summary(fit) ## ## Call: ## lm(formula = y ~ PC1_c * PC2_c, data = data_pca_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -680.91 -290.30 -93.65 143.17 1865.01 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 564.670 34.065 16.576 \u0026lt;2e-16 *** ## PC1_c -6.185 6.845 -0.904 0.3674 ## PC2_c -31.236 13.778 -2.267 0.0246 * ## PC1_c:PC2_c 4.184 2.716 1.540 0.1253 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 458.3 on 177 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.07875, Adjusted R-squared: 0.06313 ## F-statistic: 5.043 on 3 and 177 DF, p-value: 0.002236 # robust SEs model summary coeftest(fit, vcov = vcovHC(fit)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 564.6699 34.5618 16.3380 \u0026lt; 2e-16 *** ## PC1_c -6.1855 6.0165 -1.0281 0.30531 ## PC2_c -31.2358 18.7447 -1.6664 0.09741 . ## PC1_c:PC2_c 4.1836 2.9188 1.4333 0.15353 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 # interaction plot for PC1_c, PC2_c interact_plot(fit, pred = PC1_c, modx = PC2_c, data = data_pca_df) OS.time =564.67 - 6.185(PC1_c) - 31.236(PC2_c) + 4.184(PC1_c * PC2_c)\nIt’s clear from the QQ plot that my data is not normal at all. That said, the BP Test and even distribution of residuals around 0 prove show that the data is homoskedastic and exhibits a linear trend.\nFor samples of average PC1 and PC2, the predicted overall survival time is 564.67 days. PC1_c has a coefficient of -6.185. This means that for every one-unit increase in PC1_c, OS.time decreases by 6.185, on average. (not significant) PC2_c has a coefficient of -31.236. This means that for every one-unit increase in PC2_c, OS.time decreases by 31.236, on average. (significant) The coefficent for PC1_c:PC2_c is 4.184. This shows that as PC2_c increases, the effect of PC1_c on OS.time becomes more positive. (not significant)\nThis model has an adjusted R-squared of 0.06313; this means that my model explains 6.313% of the variation in OS.time\nThe Robust SE LM returned no significant coefficients save for the intercept. This means that because Robust SEs were used, PC2_c is no longer a significant predictor of Head of Pancreas.\n 4a. Bootstrapped SE LM set.seed(1) samp_distn \u0026lt;- replicate(5000, { boot_dat \u0026lt;- sample_frac(data_pca_df, replace = T) fit \u0026lt;- lm(y ~ PC1_c * PC2_c, data = boot_dat) coef(fit) }) samp_distn %\u0026gt;% t %\u0026gt;% as.data.frame %\u0026gt;% summarize_all(sd) ## (Intercept) PC1_c PC2_c PC1_c:PC2_c ## 1 33.60516 6.041935 18.34254 2.882069 samp_distn %\u0026gt;% t %\u0026gt;% as.data.frame %\u0026gt;% pivot_longer(everything()) %\u0026gt;% group_by(name) %\u0026gt;% summarize(lower = quantile(value, 0.025), upper = quantile(value, 0.975)) ## # A tibble: 4 x 3 ## name lower upper ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 500. 631. ## 2 PC1_c -17.2 6.12 ## 3 PC1_c:PC2_c -1.79 9.72 ## 4 PC2_c -66.0 5.87 The intercept’s 95 CI does not cross 0 and is significant. All other 95 CIs include 0 and indicate that the other variables are not significant predictors. This yielded the same results as the Robust SEs LM.\n 5a. Define class_diag and conf_matrix functions with optional decision threshold parameters class_diag \u0026lt;- function(probs, truth, thresh = 0.5) { tab \u0026lt;- table(factor(probs \u0026gt; thresh, levels = c(\u0026quot;FALSE\u0026quot;, \u0026quot;TRUE\u0026quot;)), truth) acc = sum(diag(tab))/sum(tab) sens = tab[2, 2]/colSums(tab)[2] spec = tab[1, 1]/colSums(tab)[1] ppv = tab[2, 2]/rowSums(tab)[2] f1 = 2 * (sens * ppv)/(sens + ppv) if (is.numeric(truth) == FALSE \u0026amp; is.logical(truth) == FALSE) { truth \u0026lt;- as.numeric(truth) - 1 } # CALCULATE EXACT AUC ord \u0026lt;- order(probs, decreasing = TRUE) probs \u0026lt;- probs[ord] truth \u0026lt;- truth[ord] TPR = cumsum(truth)/max(1, sum(truth)) FPR = cumsum(!truth)/max(1, sum(!truth)) dup \u0026lt;- c(probs[-1] \u0026gt;= probs[-length(probs)], FALSE) TPR \u0026lt;- c(0, TPR[!dup], 1) FPR \u0026lt;- c(0, FPR[!dup], 1) n \u0026lt;- length(TPR) auc \u0026lt;- sum(((TPR[-1] + TPR[-n])/2) * (FPR[-1] - FPR[-n])) data.frame(acc, sens, spec, ppv, f1, auc) } # prints a confusion matrix to the screen conf_matrix \u0026lt;- function(probs, truth, thresh = 0.5) { table(factor(probs \u0026gt; thresh, levels = c(\u0026quot;FALSE\u0026quot;, \u0026quot;TRUE\u0026quot;)), truth) } # find optimal thresh optimizes the decision # threshold to yield the maximum f1 score find_optimal_thresh \u0026lt;- function(fit, data_genes, probs, plot = FALSE) { data_genes$probs \u0026lt;- probs # initialize f_df variable f_df \u0026lt;- NULL # for i in 1000 iterations, get the f1-score for # every possible cutoff between 0 and 1, # incrementing 0.001 each iteration for (i in 1:1000) { f_df \u0026lt;- rbind(f_df, data.frame(cutoff = i/1000, f1 = class_diag(data_genes$probs, data_genes$Head_of_pancreas, i/1000)$f1)) } # get decision threshold that yielded the highest # F1-score thresh \u0026lt;- (f_df %\u0026gt;% arrange(desc(f1)))[1, ] %\u0026gt;% pull(cutoff) if (plot == TRUE) { print(ggplot(f_df, aes(cutoff, f1)) + geom_line() + geom_vline(aes(xintercept = thresh)) + xlab(\u0026quot;Decision Threshold\u0026quot;) + ylab(\u0026quot;F1-score\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;Identifying the Optimal Decision Threshold\u0026quot;)) } return(thresh) } # generate density plot separated by # head_of_pancreas group logit_density \u0026lt;- function(fit, Head_of_pancreas) { # plot frequency plot of logit for both classes print(data.frame(predict = predict(fit, type = \u0026quot;link\u0026quot;), Head_of_pancreas = Head_of_pancreas) %\u0026gt;% ggplot(aes(predict)) + geom_density(aes(fill = Head_of_pancreas), alpha = 0.5) + theme_minimal() + geom_vline(aes(xintercept = 0)) + xlab(\u0026quot;logit\u0026quot;)) }  5b. Logistic Regression Predicting Head of Pancreas from Three Randomly Picked Genes set.seed(5) data_genes \u0026lt;- data %\u0026gt;% select(Head_of_pancreas, contains(\u0026quot;ENSG\u0026quot;)) data_genes \u0026lt;- data_genes %\u0026gt;% select(sample(2:ncol(data_genes), 3), Head_of_pancreas) fit \u0026lt;- glm(Head_of_pancreas ~ ., family = \u0026quot;binomial\u0026quot;, data = data_genes, control = list(maxit = 75)) # get class probabilities data_genes$probs \u0026lt;- predict(fit, type = \u0026quot;response\u0026quot;) summary(fit) ## ## Call: ## glm(formula = Head_of_pancreas ~ ., family = \u0026quot;binomial\u0026quot;, data = data_genes, ## control = list(maxit = 75)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9616 -1.3477 0.6723 0.9301 1.0202 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 0.39463 0.24572 1.606 0.108 ## ENSG00000169347.15 0.07724 0.13142 0.588 0.557 ## ENSG00000179751.6 0.17209 0.28277 0.609 0.543 ## ENSG00000142615.7 -0.10031 0.25401 -0.395 0.693 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 217.77 on 181 degrees of freedom ## Residual deviance: 208.19 on 178 degrees of freedom ## AIC: 216.19 ## ## Number of Fisher Scoring iterations: 4 exp(coef(fit)) ## (Intercept) ENSG00000169347.15 ENSG00000179751.6 ENSG00000142615.7 ## 1.4838370 1.0803008 1.1877791 0.9045531 thresh \u0026lt;- find_optimal_thresh(fit, data_genes, data_genes$probs, TRUE) # get relevant metrics on fit class_diag(data_genes$probs, data$Head_of_pancreas, thresh) ## acc sens spec ppv f1 auc ## 1 0.7417582 0.9923077 0.1153846 0.7371429 0.8459016 0.6445266 # generate confusion matrix of fit conf_matrix(data_genes$probs, data$Head_of_pancreas, thresh) ## truth ## 0 1 ## FALSE 6 1 ## TRUE 46 129 logit_density(fit, data_genes$Head_of_pancreas) ggplot(data_genes) + geom_roc(aes(d = as.numeric(Head_of_pancreas), m = probs), n.cuts = 0) For this one I picked three random genes with which to work.\nlog(odds of class 1) = 0.07724(ENSG00000169347.15) - 0.17209(ENSG00000175535.6) + 0.10031(ENSG00000164266.9) + 0.39463\nLog odds are hard to intrepret, so I exponentiate all of the coefficients.\nControlling for ENSG00000175535.6 and ENSG00000164266.9, each one unit increase in ENSG00000169347.15 increases the odds of being class 1 by a factor of 1.0803008 (not significant)\nControlling for ENSG00000169347.15 and ENSG00000164266.9, each one unit increase in ENSG00000175535.6 increases the odds of being class 1 by a factor of 1.1877791 (not significant)\nControlling for ENSG00000169347.15 ENSG00000175535.6, each one unit increase in ENSG00000164266.9 increases the odds of being class 1 by a factor of 0.9045531 (not significant)\nImmediately, it’s clear that this model sacrifices specificity (0.1153846) for sensitivity (0.9923077). The model’s accuracy is 0.7417582 and its precision is 0.7371429 The F1-score is surprisingly high (0.8459016), which is never bad. The AUC is a lousy 0.6445266.\nThe ROC curve has a very low AUC (0.6445266). This means that the model cannot achieve a high TPR without incurring a high FPR.\n 6a. Predicting Head of Pancreas from All Genes # pull out genes, head of pancreas data_genes \u0026lt;- data %\u0026gt;% select(contains(\u0026quot;ENSG\u0026quot;), Head_of_pancreas) # fit logistic model to all genes; predict log odds # of Head_of_pancreas; maxit = 75 prevents model # from failing with large numbers of variables fit \u0026lt;- glm(Head_of_pancreas ~ ., family = \u0026quot;binomial\u0026quot;, data = data_genes, control = list(maxit = 75)) # get class probabilities data_genes$probs \u0026lt;- predict(fit, type = \u0026quot;response\u0026quot;) summary(fit) ## ## Call: ## glm(formula = Head_of_pancreas ~ ., family = \u0026quot;binomial\u0026quot;, data = data_genes, ## control = list(maxit = 75)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.85033 -0.05722 0.11068 0.48014 1.84377 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.563394 2.983896 -0.524 0.60032 ## ENSG00000164266.9 0.102611 0.277310 0.370 0.71136 ## ENSG00000169347.15 0.344227 0.418384 0.823 0.41065 ## ENSG00000175535.6 -0.667590 1.139099 -0.586 0.55783 ## ENSG00000215704.8 1.631599 1.326965 1.230 0.21886 ## ENSG00000211892.3 -0.027101 0.226791 -0.119 0.90488 ## ENSG00000243480.6 -1.252604 0.643884 -1.945 0.05173 . ## ENSG00000096006.10 -0.307265 0.267720 -1.148 0.25109 ## ENSG00000118271.8 0.671077 0.707698 0.948 0.34300 ## ENSG00000219073.6 -0.824490 1.552025 -0.531 0.59526 ## ENSG00000066405.11 -0.555978 0.335348 -1.658 0.09733 . ## ENSG00000122711.7 -0.026514 0.288477 -0.092 0.92677 ## ENSG00000108849.6 0.145380 0.196099 0.741 0.45848 ## ENSG00000170890.12 -2.180794 1.493387 -1.460 0.14421 ## ENSG00000256618.2 0.034362 0.158207 0.217 0.82805 ## ENSG00000175084.10 0.186929 0.189869 0.985 0.32486 ## ENSG00000254647.5 0.476140 0.325111 1.465 0.14304 ## ENSG00000134193.13 0.512320 0.248319 2.063 0.03910 * ## ENSG00000275896.3 -0.096492 0.460265 -0.210 0.83395 ## ENSG00000167653.4 0.089175 0.156002 0.572 0.56757 ## [ reached getOption(\u0026quot;max.print\u0026quot;) -- omitted 42 rows ] ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 217.77 on 181 degrees of freedom ## Residual deviance: 104.19 on 120 degrees of freedom ## AIC: 228.19 ## ## Number of Fisher Scoring iterations: 8 thresh \u0026lt;- find_optimal_thresh(fit, data_genes, data_genes$probs, TRUE) # get relevant metrics on fit class_diag(data_genes$probs, data$Head_of_pancreas, thresh) ## acc sens spec ppv f1 auc ## 1 0.8901099 0.9538462 0.7307692 0.8985507 0.9253731 0.9322485 # generate confusion matrix of fit conf_matrix(data_genes$probs, data$Head_of_pancreas, thresh) ## truth ## 0 1 ## FALSE 38 6 ## TRUE 14 124 # generate ROC curve for fit ggplot(data_genes) + geom_roc(aes(d = as.numeric(Head_of_pancreas), m = probs), n.cuts = 0) logit_density(fit, data_genes$Head_of_pancreas) There are too many coefficients to count, but I’ll provide a template for their interpretation: variable has a coefficient of coeff. This means that for every one-unit increase in variable, log odds of head of pancreas (increases/decreases) by coeff. (if Pr(\u0026gt;|z|) \u0026lt; 0.05: significant; else: not significant)\nThe model classifies 38 of the 52 false values as false. It classifies 124 of the 130 positive values as positive.\nThe model performs very well. It is somewhat more sensitive (0.95) than it is specific (0.73); it gets a high accuracy (0.89); it has a precision of 0.899; it has a great f1 (0.925). The AUC is a great 0.932.\nThe optimal decision threshold for this model is at probability = 0.498; anything above is classified a Head of Pancreas sample. Anything below is non-head of pancreas.\nThe ROC curve has a very large AUC (0.932). This means that the model can achieve a very high TPR while maintaining a relatively low FPR.\n 6b. 10 fold CV with all variables data_genes \u0026lt;- data %\u0026gt;% select(contains(\u0026quot;ENSG\u0026quot;), Head_of_pancreas) k = 10 temp \u0026lt;- data_genes[sample(nrow(data_genes)), ] folds \u0026lt;- cut(seq(1:nrow(data_genes)), breaks = k, labels = F) diags \u0026lt;- NULL for (i in 1:k) { train \u0026lt;- temp[folds != i, ] #create training set (all but fold i) test \u0026lt;- temp[folds == i, ] #create test set (just fold i) truth \u0026lt;- test$Head_of_pancreas #save truth labels from fold i fit \u0026lt;- glm(Head_of_pancreas ~ ., family = \u0026quot;binomial\u0026quot;, data = train, control = list(maxit = 75)) probs \u0026lt;- predict(fit, newdata = test, type = \u0026quot;response\u0026quot;) thresh \u0026lt;- find_optimal_thresh(fit, train, predict(fit, type = \u0026quot;response\u0026quot;)) diags \u0026lt;- rbind(diags, class_diag(probs, truth, thresh)) } summarize_all(diags, mean) ## acc sens spec ppv f1 auc ## 1 0.6312865 0.7257792 0.3980952 0.7486476 0.731227 0.5923399 The 10-fold CV has exposed the degree to which the previous model had overfitted to the dataset. Even with optimal decision cutoffs, the AUC was only 0.632. Accuracy, sensitivity, specificity, precision, and f1 are 0.7186647, 0.442619, 0.7710556, and 0.7384776, respectively.\n 6c. LASSO set.seed(1234) data_genes \u0026lt;- data %\u0026gt;% select(contains(\u0026quot;ENSG\u0026quot;), Head_of_pancreas) y \u0026lt;- as.matrix(data_genes$Head_of_pancreas) preds \u0026lt;- model.matrix(Head_of_pancreas ~ ., data = data_genes)[, -1] %\u0026gt;% scale cv \u0026lt;- cv.glmnet(preds, y, family = \u0026quot;binomial\u0026quot;) { plot(cv$glmnet.fit, \u0026quot;lambda\u0026quot;, label = TRUE) abline(v = log(cv$lambda.1se)) } lasso_fit \u0026lt;- glmnet(preds, y, family = \u0026quot;binomial\u0026quot;, lambda = cv$lambda.1se) # probs \u0026lt;- predict(lasso_fit, preds, # type=\u0026#39;response\u0026#39;) # class_diag(probs,data_genes$Head_of_pancreas,0.5) # conf_matrix(probs,data_genes$Head_of_pancreas) x \u0026lt;- coef(lasso_fit) rownames(x)[x[, 1] \u0026gt; 0] ## [1] \u0026quot;(Intercept)\u0026quot; \u0026quot;ENSG00000108849.6\u0026quot; \u0026quot;ENSG00000175084.10\u0026quot; ## [4] \u0026quot;ENSG00000188257.9\u0026quot; \u0026quot;ENSG00000172016.14\u0026quot; LASSO selected four genes as effective predictors:“ENSG00000108849.6” “ENSG00000175084.10” “ENSG00000188257.9” “ENSG00000172016.14”. Using these variables to train a model will make it much more generalizable.\n 6d. 10-fold CV on LASSO-honed Logistic Model data_genes \u0026lt;- data %\u0026gt;% select(ENSG00000108849.6, ENSG00000175084.10, ENSG00000188257.9, ENSG00000172016.14, Head_of_pancreas) k = 10 temp \u0026lt;- data_genes[sample(nrow(data_genes)), ] folds \u0026lt;- cut(seq(1:nrow(data_genes)), breaks = k, labels = F) diags \u0026lt;- NULL for (i in 1:k) { train \u0026lt;- temp[folds != i, ] #create training set (all but fold i) test \u0026lt;- temp[folds == i, ] #create test set (just fold i) truth \u0026lt;- test$Head_of_pancreas #save truth labels from fold i fit \u0026lt;- glm(Head_of_pancreas ~ ., family = \u0026quot;binomial\u0026quot;, data = train) probs \u0026lt;- predict(fit, newdata = test, type = \u0026quot;response\u0026quot;) thresh \u0026lt;- find_optimal_thresh(fit, train, predict(fit, type = \u0026quot;response\u0026quot;)) diags \u0026lt;- rbind(diags, class_diag(probs, truth, thresh)) } summarize_all(diags, mean) ## acc sens spec ppv f1 auc ## 1 0.7312865 0.9149567 0.2194444 0.745767 0.8189543 0.6573088 I suppose this is the best I can do given the tools we’ve learned in this course. The AUC is terrible (0.680). While sensitivity is great (0.93), specificity is non-existent (0.1). This tells me that the decision threshold chosen for my model is basically near 0, and it is classifying everything as Head of pancreas. Accuracy is bad, but not atrocious (0.69). Precision and f1 score are 0.72 and 0.81, respectively.\nAlthough this model gets lower scores than some of its predecessors, it is much more generalizable than those previous models and will thus perform better than them on new data.\n## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.15.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] rstatix_0.5.0 interactions_1.1.3 factoextra_1.0.7 ## [4] circlize_0.4.9 ComplexHeatmap_2.0.0 reshape2_1.4.4 ## [7] prettyR_2.2-3 glmnet_4.0-2 Matrix_1.2-18 ## [10] plotROC_2.2.1 lmtest_0.9-38 zoo_1.8-8 ## [13] sandwich_3.0-0 forcats_0.5.0 stringr_1.4.0 ## [16] dplyr_1.0.0 purrr_0.3.4 tidyr_1.1.0 ## [19] tibble_3.0.1 ggplot2_3.3.1 tidyverse_1.3.0 ## [22] readr_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-148 fs_1.4.1 lubridate_1.7.9 ## [4] RColorBrewer_1.1-2 httr_1.4.1 tools_3.6.0 ## [7] backports_1.1.7 utf8_1.1.4 R6_2.4.1 ## [10] mgcv_1.8-31 DBI_1.1.0 colorspace_1.4-1 ## [13] GetoptLong_0.1.8 withr_2.2.0 tidyselect_1.1.0 ## [16] curl_4.3 compiler_3.6.0 cli_2.0.2 ## [19] rvest_0.3.5 formatR_1.7 xml2_1.3.2 ## [22] labeling_0.3 bookdown_0.19 scales_1.1.1 ## [25] digest_0.6.25 foreign_0.8-72 rmarkdown_2.2 ## [28] rio_0.5.16 pkgconfig_2.0.3 htmltools_0.4.0 ## [31] dbplyr_1.4.4 rlang_0.4.6 GlobalOptions_0.1.1 ## [34] readxl_1.3.1 rstudioapi_0.11 farver_2.0.3 ## [37] shape_1.4.4 generics_0.0.2 jsonlite_1.6.1 ## [40] zip_2.0.4 car_3.0-8 magrittr_1.5 ## [43] Rcpp_1.0.5 munsell_0.5.0 fansi_0.4.1 ## [46] abind_1.4-5 lifecycle_0.2.0 stringi_1.4.6 ## [49] yaml_2.2.1 carData_3.0-4 plyr_1.8.6 ## [52] blob_1.2.1 parallel_3.6.0 ggrepel_0.8.2 ## [55] crayon_1.3.4 lattice_0.20-41 haven_2.3.1 ## [58] splines_3.6.0 jtools_2.1.0 pander_0.6.3 ## [61] hms_0.5.3 knitr_1.28 pillar_1.4.4 ## [64] rjson_0.2.20 codetools_0.2-16 reprex_0.3.0 ## [67] glue_1.4.1 evaluate_0.14 blogdown_0.19 ## [70] data.table_1.12.8 modelr_0.1.8 png_0.1-7 ## [73] vctrs_0.3.1 foreach_1.5.0 cellranger_1.1.0 ## [76] gtable_0.3.0 clue_0.3-57 assertthat_0.2.1 ## [79] openxlsx_4.1.5 xfun_0.14 broom_0.5.6 ## [82] survival_3.1-12 iterators_1.0.12 cluster_2.1.0 ## [85] ellipsis_0.3.1 ## [1] \u0026quot;2020-12-08 23:18:44 CST\u0026quot; ## sysname ## \u0026quot;Darwin\u0026quot; ## release ## \u0026quot;19.6.0\u0026quot; ## version ## \u0026quot;Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64\u0026quot; ## nodename ## \u0026quot;Extension.local\u0026quot; ## machine ## \u0026quot;x86_64\u0026quot; ## login ## \u0026quot;root\u0026quot; ## user ## \u0026quot;ryanbailey\u0026quot; ## effective_user ## \u0026quot;ryanbailey\u0026quot;  ","date":1606348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606348800,"objectID":"3222125a50061c479434e3d317e32f9a","permalink":"/projects/project2/","publishdate":"2020-11-26T00:00:00Z","relpermalink":"/projects/project2/","section":"projects","summary":"REB3566  Welcome to for loop h*ll  0a. Introduction This analysis is based on The Cancer Genome Atlas’ Pancreatic Adenocarcinoma Project (TCGA-PAAD). TCGA provides large, well-documented cancer datasets that are semi open-source.","tags":null,"title":"Fitting Models on High Dimensional Biological Data (in R)","type":"projects"},{"authors":["Ryan Bailey"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ae53b01876c846a48fd39eb0ec4d78ac","permalink":"/books/cancerland/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/books/cancerland/","section":"books","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Cancerland","type":"books"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"6a3e8097b93c27614c34af0e6f30bde2","permalink":"/books/gunsgermssteel/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/books/gunsgermssteel/","section":"books","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Guns, Germs, and Steel","type":"books"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2d8caccaf87e3ba7e26a6437c719ea51","permalink":"/books/deathofexpertise/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/books/deathofexpertise/","section":"books","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"The Death of Expertise","type":"books"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2a09bb58f893cb3a8c069ca85046a171","permalink":"/books/doomsdaymachine/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/books/doomsdaymachine/","section":"books","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"The Doomsday Machine","type":"books"},{"authors":["Ryan Bailey","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Ryan Bailey","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"         EID: REB3566  0. Introduction Three datasets are combined and explored in this R markdown file. The first dataset contains cancer mortality and incidence by states in the United States. The second dataset has relevant religiosity rankings (number of congregations, number of adherents per 1,000 people) by state. The final dataset lists each state by region (West, Southeast, Southwest, Northeast, and Midwest). The first two datasets were downloaded from data.world. The Regions dataset was constructed by hand from information found on ducksters.com/geography.\nI would like to investigate relationships between state/regional religiosity and cancer rates and incidences. I do not expect to find any prominent associations within the data. The datasets neglect dozens (if not hundreds) of potential confounds and biases. If cancer incidence or mortality rates are correlated with religiosity, this is likely because Bible Belt states have more inadequate healthcare than other US regions. Mortality and incidence may differ to some extent by region, but this difference is unlikely to be statistically significant.\nImport libraries # specific data import instructions library(readr) # data wrangling library(tidyverse) # plotting library(ggplot2) library(plotly) library(GGally) # for making clustered correlation heatmap library(prettyR) library(reshape2) library(ComplexHeatmap) library(circlize) # for Kmeans and PAM Clustering library(cluster) # for PCA Biplot library(factoextra) # for Pretty Workflows library(DiagrammeR)  Workflow I realize that there’s a lot going on with my Project. It’s very figure-heavy, which I know makes it feel more cluttered; I just hope makes it also makes it more informative. I included my workflow to try and clarify my process and to let you know what things to expect and the order to expect them in. It was also a really cool opportunity to use DiagrameR.\ngrViz(\u0026quot;digraph flowchart { # node definitions with substituted label text node [fontname = Helvetica, shape = rectangle] tab1 [label = \u0026#39;@@1\u0026#39;] tab2 [label = \u0026#39;@@2\u0026#39;] tab3 [label = \u0026#39;@@3\u0026#39;] tab4 [label = \u0026#39;@@4\u0026#39;] tab5 [label = \u0026#39;@@5\u0026#39;] tab6 [label = \u0026#39;@@6\u0026#39;] tab7 [label = \u0026#39;@@7\u0026#39;] tab8 [label = \u0026#39;@@8\u0026#39;] tab9 [label = \u0026#39;@@9\u0026#39;] tab10 [label = \u0026#39;@@10\u0026#39;] tab11 [label = \u0026#39;@@11\u0026#39;] tab12 [label = \u0026#39;@@12\u0026#39;] tab13 [label = \u0026#39;@@13\u0026#39;] tab14 [label = \u0026#39;@@14\u0026#39;] tab15 [label = \u0026#39;@@15\u0026#39;] # edge definitions with the node IDs tab1 -\u0026gt; tab2 -\u0026gt; tab3 -\u0026gt; tab4 -\u0026gt; tab5 -\u0026gt; tab6 -\u0026gt; tab7 -\u0026gt; tab8; tab8 -\u0026gt; tab9; tab8 -\u0026gt; tab10 -\u0026gt; tab12 -\u0026gt; tab13; tab8 -\u0026gt; tab11 -\u0026gt; tab14 -\u0026gt; tab15; tab9 -\u0026gt; tab13; tab9 -\u0026gt; tab15 } [1]: \u0026#39;Import libraries\u0026#39; [2]: \u0026#39;Import All 3 Datasets\u0026#39; [3]: \u0026#39;Join All 3 Datasets\u0026#39; [4]: \u0026#39;Drop Irrelevant Variables from Joined Dataset\u0026#39; [5]: \u0026#39;Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot\u0026#39; [6]: \u0026#39;Visualize Different Variables and the Relationships Between Them\u0026#39; [7]: \u0026#39;Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot\u0026#39; [8]: \u0026#39;Extract Numeric Variables From Dataset\u0026#39; [9]: \u0026#39;Run PCA Only on Numeric Variables\u0026#39; [10]: \u0026#39;Validate Optimal K for Kmeans\u0026#39; [11]: \u0026#39;Validate Optimal K for PAM\u0026#39; [12]: \u0026#39;Perform Kmeans Clustering Using Optimal K\u0026#39; [13]: \u0026#39;Visualize Kmeans Clusters In Feature Space\u0026#39; [14]: \u0026#39;Perform PAM Clustering Using Optimal K\u0026#39; [15]: \u0026#39;Visualize PAM Clusters In Feature Space\u0026#39; \u0026quot;)  {\"x\":{\"diagram\":\"digraph flowchart {\\n # node definitions with substituted label text\\n node [fontname = Helvetica, shape = rectangle] \\n tab1 [label = \\\"Import libraries\\\"]\\n tab2 [label = \\\"Import All 3 Datasets\\\"]\\n tab3 [label = \\\"Join All 3 Datasets\\\"]\\n tab4 [label = \\\"Drop Irrelevant Variables from Joined Dataset\\\"]\\n tab5 [label = \\\"Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot\\\"]\\n tab6 [label = \\\"Visualize Different Variables and the Relationships Between Them\\\"]\\n tab7 [label = \\\"Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot\\\"]\\n tab8 [label = \\\"Extract Numeric Variables From Dataset\\\"]\\n tab9 [label = \\\"Run PCA Only on Numeric Variables\\\"]\\n tab10 [label = \\\"Validate Optimal K for Kmeans\\\"]\\n tab11 [label = \\\"Validate Optimal K for PAM\\\"]\\n tab12 [label = \\\"Perform Kmeans Clustering Using Optimal K\\\"]\\n tab13 [label = \\\"Visualize Kmeans Clusters In Feature Space\\\"]\\n tab14 [label = \\\"Perform PAM Clustering Using Optimal K\\\"]\\n tab15 [label = \\\"Visualize PAM Clusters In Feature Space\\\"]\\n\\n # edge definitions with the node IDs\\n tab1 - tab2 - tab3 - tab4 - tab5 - tab6 - tab7 - tab8;\\n tab8 - tab9;\\n tab8 - tab10 - tab12 - tab13;\\n tab8 - tab11 - tab14 - tab15;\\n tab9 - tab13;\\n tab9 - tab15\\n }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}  Import datasets # read datasets cancer_states \u0026lt;- read_csv(\u0026quot;datasets/cancer_states.csv\u0026quot;) religion_states \u0026lt;- read_csv(\u0026quot;datasets/religion_states.csv\u0026quot;) region_states \u0026lt;- read_csv(\u0026quot;datasets/region_states.csv\u0026quot;) # dataset dimensions cancer_states %\u0026gt;% dim ## [1] 50 5 religion_states %\u0026gt;% dim ## [1] 50 8 region_states %\u0026gt;% dim ## [1] 50 2   2. Joining/Merging Since I have three datasets, I have to perform two joins. I used inner_join() for both because I only want complete cases. NAs were not a problem as the datasets were relatively small, had no missing values, and used the same identifiers for states.\n2a. Joining Datasets # none of the datasets have a colname for the first column; join each dataset by this column data \u0026lt;- inner_join(cancer_states, religion_states, by = c(\u0026quot;X1\u0026quot; = \u0026quot;X1\u0026quot;)) %\u0026gt;% inner_join(region_states, by = c(\u0026quot;X1\u0026quot; = \u0026quot;X1\u0026quot;)) data %\u0026gt;% glimpse ## Rows: 50 ## Columns: 13 ## $ X1 \u0026lt;chr\u0026gt; \u0026quot;Alaska\u0026quot;, \u0026quot;Alabama\u0026quot;, \u0026quot;Arkansa… ## $ mo_Rate \u0026lt;dbl\u0026gt; 173.1, 182.1, 189.6, 146.4, 1… ## $ mo_range_low \u0026lt;dbl\u0026gt; 164.2, 174.5, 174.5, 127.9, 1… ## $ mo_range_high \u0026lt;dbl\u0026gt; 174.4, 199.3, 199.3, 155.3, 1… ## $ in_Rate \u0026lt;dbl\u0026gt; 406.6, 437.9, 456.2, 379.8, 3… ## $ in_range_low \u0026lt;dbl\u0026gt; 369.9, 420.4, 447.0, 369.9, 3… ## $ in_range_high \u0026lt;dbl\u0026gt; 416.5, 445.7, 461.0, 416.5, 4… ## $ total_number_of_congregations \u0026lt;dbl\u0026gt; 1246, 10514, 6697, 4673, 2355… ## $ total_number_of_adherents \u0026lt;dbl\u0026gt; 240833, 3007553, 1614357, 237… ## $ rates_of_adherence_per_1_000_population \u0026lt;dbl\u0026gt; 339.09, 629.23, 553.64, 372.3… ## $ of_adherents \u0026lt;dbl\u0026gt; 0.33909, 0.62923, 0.55364, 0.… ## $ of_adults_who_are_highly_religious \u0026lt;dbl\u0026gt; 0.45, 0.77, 0.70, 0.53, 0.49,… ## $ Region \u0026lt;chr\u0026gt; \u0026quot;West\u0026quot;, \u0026quot;Southeast\u0026quot;, \u0026quot;Southea…   1 and 3: Tidying and Wrangling I perform data tidying after joining. Here I use pivot functions to make summary statistics more digestible. I use pivot_longer to reformat the summarized output from a single row with ~54 variables (one for each variable and each summary stat) to one with 3 variables (variable, stat, and value) and 54 rows. I pivot wider to give a final table of 8 variables (each numeric variable in the dataset) and 9 rows (each type of summary statistic used).\nI also drop mo/in_range_high/low and total number of religious adherence. These first four variables correspond to the mortality and incidence 95% confidence intervals. The dataset contains a rate of adherence variable that makes total adherence obsolete (due to its bias towards more populous states).\n3a. Summary Statistics - 6 Core dplyr Functions # xx_range_high/low are just the 95% CI for the mortality and incidence rates # total number of adherents is highly dependent on state population; this is dropped because there is a rates of adherence per 1000 people variable data \u0026lt;- data %\u0026gt;% select(!c(mo_range_low,mo_range_high,in_range_low,in_range_high, total_number_of_adherents)) # use of all 6 core dplyr functions data %\u0026gt;% group_by(Region) %\u0026gt;% summarize(total_cong = sum(total_number_of_congregations), mean_cong = mean(total_number_of_congregations), sd_cong = sd(total_number_of_congregations)) %\u0026gt;% filter(Region != \u0026quot;West\u0026quot;) %\u0026gt;% select(!c(total_cong)) %\u0026gt;% arrange(mean_cong) %\u0026gt;% mutate(plus_one_sd = mean_cong + sd_cong,minus_one_sd = mean_cong - sd_cong ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 5 ## Region mean_cong sd_cong plus_one_sd minus_one_sd ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Northeast 4789. 5276. 10065. -487. ## 2 Midwest 6812. 3957. 10769. 2854. ## 3 Southeast 9608 3683. 13291. 5925. ## 4 Southwest 10506. 11713. 22220. -1207.  1a and 3b: Summary Statistics - Summarize # This is an infinitely wordier way to create the summary tables I love in Python Pandas; every variable gets a full suite of summary statistics N \u0026lt;- nrow(data) data %\u0026gt;% summarize_if(is.numeric,list(xxxmean = mean, xxxsd = sd , xxxse = function(x) sd(x) / sqrt(N), xxxmin = min,xxxper25 = function(x) quantile(x,0.25), xxxmedian = median,xxxper75 = function(x) quantile(x, 0.75),xxxmax = max, xxxrange = function(x) max(x) - min(x))) %\u0026gt;% pivot_longer(cols = everything()) %\u0026gt;% separate(name, into = c(\u0026quot;variable\u0026quot;, \u0026quot;stat\u0026quot;), sep = \u0026quot;_xxx\u0026quot;) %\u0026gt;% pivot_wider(names_from = variable, values_from = value) ## # A tibble: 9 x 7 ## stat mo_Rate in_Rate total_number_of… rates_of_adhere… of_adherents ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 mean 165. 440. 6886. 483. 0.483 ## 2 sd 15.6 31.1 5840. 103. 0.103 ## 3 se 2.20 4.39 826. 14.6 0.0146 ## 4 min 128. 370. 677 276. 0.276 ## 5 per25 155. 417. 2432 400. 0.400 ## 6 medi… 164. 446. 5699 507. 0.507 ## 7 per75 174. 461. 9406 553. 0.553 ## 8 max 199. 514. 27848 791. 0.791 ## 9 range 71.4 144. 27171 515. 0.515 ## # … with 1 more variable: of_adults_who_are_highly_religious \u0026lt;dbl\u0026gt; data %\u0026gt;% group_by(Region) %\u0026gt;% summarize_if(is.numeric,list(xxxmean = mean, xxxsd = sd , xxxse = function(x) sd(x) / sqrt(N), xxxmin = min,xxxper25 = function(x) quantile(x,0.25), xxxmedian = median,xxxper75 = function(x) quantile(x, 0.75),xxxmax = max, xxxrange = function(x) max(x) - min(x))) %\u0026gt;% pivot_longer(cols = !Region) %\u0026gt;% separate(name, into = c(\u0026quot;variable\u0026quot;, \u0026quot;stat\u0026quot;), sep = \u0026quot;_xxx\u0026quot;) %\u0026gt;% pivot_wider(names_from = variable, values_from = value) ## # A tibble: 45 x 8 ## Region stat mo_Rate in_Rate total_number_of… rates_of_adhere… of_adherents ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Midwe… mean 166. 450. 6812. 526. 0.526 ## 2 Midwe… sd 9.85 11.6 3957. 70.5 0.0705 ## 3 Midwe… se 1.39 1.65 560. 9.97 0.00997 ## 4 Midwe… min 151. 431. 1498 421. 0.421 ## 5 Midwe… per25 159. 442. 4302. 480. 0.480 ## 6 Midwe… medi… 166. 450. 6016. 537. 0.537 ## 7 Midwe… per75 173. 458. 9176 558. 0.558 ## 8 Midwe… max 179. 472. 13606 671. 0.671 ## 9 Midwe… range 28.6 40.5 12108 250. 0.250 ## 10 North… mean 163. 466. 4789. 457. 0.457 ## # … with 35 more rows, and 1 more variable: ## # of_adults_who_are_highly_religious \u0026lt;dbl\u0026gt;   4. Visualizing GGPairs, A Hierarchically Clustered Correlation Heatmap, Faceted Boxplot, Violin Plot, and Faceted Scatterplot (Bubbleplot)  4a. GGPairs Plot to View Relationships Between Each Variables # pairwise summary plots like ggpairs are great for getting a mile-high view of the dataset. They fall apart when you get more than 8 or 9 variables. data %\u0026gt;% select(!X1) %\u0026gt;% ggpairs(aes(color = as.factor(data$Region)), progress = F) Few significant relationships jump out in the GGPairs plot. There appears to be a significant, strong, and positive correlation between cancer mortality rate and the total number of congregations in the midwest. Also, there seems to be a significant, strong, and negative correlation between cancer mortality rate and adherence rate per 1000 people in the midwest.\n 4b Hierarchically Clustered Correlation Heatmap # I personally find correlation geom_tiles hard to read. I found this package called ComplexHeatmap which will Hierarchically cluster the correlation values. This places similar variable combinations together and makes correlations quite a bit easier to assess. # define color function col_fun = colorRamp2(c(-1, 0, 1), c(\u0026quot;green\u0026quot;, \u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;)) # generate a clustered heatmap for the correlation matrix Heatmap(data %\u0026gt;% select_if(is.numeric) %\u0026gt;% cor(use=\u0026quot;pair\u0026quot;) %\u0026gt;% as.matrix, name = \u0026quot;Correlation Matrix of Numeric Variables\u0026quot;, #title of legend column_title = \u0026quot;\u0026quot;, row_title = \u0026quot;\u0026quot;, row_names_gp = gpar(fontsize = 6), column_names_gp = gpar(fontsize = 6), col = col_fun) # Text size for row names As identified in the ggpairs plot, there are few strong correlations between any variables (save for the variables that are obviously related e.g., of_adherents and rates of adherence per 1000). This will show the same thing as a geom_tile, but here, it’s a little easier to see the relationships between variables since similarly correlated variables are “clustered” closely together.\nInterestingly (or maybe not interestingly) the variable correlations cluster based on the dataset from which they originate. This is likely because mortality and incidence often go hand in hand.\n 4c. Boxplot Using stat=“summary” and facet_wrap f \u0026lt;- function(x) { r \u0026lt;- quantile(x, probs = c(0.10, 0.25, 0.5, 0.75, 0.90)) names(r) \u0026lt;- c(\u0026quot;ymin\u0026quot;, \u0026quot;lower\u0026quot;, \u0026quot;middle\u0026quot;, \u0026quot;upper\u0026quot;, \u0026quot;ymax\u0026quot;) r } order \u0026lt;- c(\u0026quot;low\u0026quot;,\u0026quot;moderate\u0026quot;,\u0026quot;high\u0026quot;) df \u0026lt;- data %\u0026gt;% mutate(mo_rate_f= as.factor(cut(mo_Rate, breaks=c(115,150,170, 250), labels = order,levels=order))) %\u0026gt;% group_by(Region) %\u0026gt;% as.data.frame df %\u0026gt;% ggplot(aes(Region,total_number_of_congregations)) + geom_boxplot(aes(fill = Region), stat = \u0026quot;summary\u0026quot;, fun.data = f, position = position_dodge(1), alpha = 1) + scale_fill_viridis_d() + theme_minimal() + ylab(\u0026quot;Total Number of Congregations\u0026quot;)+ facet_wrap(~mo_rate_f) + coord_flip() + ggtitle(\u0026quot;Total Number of Congregations By Region\u0026quot;, subtitle = \u0026quot;Faceted by low, moderate, and high mortality rates\u0026quot;) Here are horizontal boxplots comparing the number of congregations by region and low, moderate, high cancer mortality rates. The low number of samples causes some boxplots to spawn as just their median. The only stand-out difference between groups is between the West and Southeast regions in the moderate mortality rate group.\n 4d. Violin Plot data %\u0026gt;% ggplot(aes(x=Region, y=in_Rate)) + geom_violin(aes(x=Region, y=in_Rate, fill = Region) ,alpha = 0.5,trim = F, draw_quantiles=0:2/2) + geom_jitter(aes(color = rates_of_adherence_per_1_000_population),alpha = 1, size = 3) + scale_color_gradient(low = \u0026#39;blue\u0026#39;, high = \u0026#39;red\u0026#39;)+ theme_minimal() + ylab(\u0026quot;Cancer Incidence Rate\u0026quot;) + xlab(\u0026#39;Region\u0026#39;) + ggtitle(\u0026quot;Cancer Incidence by Region\u0026quot;) + scale_fill_viridis_d() This plot compares cancer incidence and rates of adherence between regions. We can see that there is a much higher cancer incidence in the Northeast than there is in the West. There is also greater rates of adherence in the Southeast than there is in the West.\n 4e. Faceted Scatterplot with linear models and custom axis ticks data %\u0026gt;% ggplot(aes(x=mo_Rate, y=rates_of_adherence_per_1_000_population)) + geom_point(aes(size = mo_Rate, color = Region),alpha = 0.5) + scale_size(range = c(.1, 7), name=\u0026quot;Cancer Incidence\u0026quot;) + geom_smooth(aes(color= Region,group=Region), method = \u0026quot;lm\u0026quot;, se=T,alpha = 0.2)+ scale_y_continuous(breaks = seq(0,700, 200), limits = c(0,700)) + scale_color_viridis_d() + theme_minimal() + facet_wrap(~Region) + xlab(\u0026quot;Mortality Rate\u0026quot;) + ylab(\u0026quot;Rates of Adherence (per 1000 people)\u0026quot;) + ggtitle(\u0026quot;Rates of Religous Adherence by Mortality Rate\u0026quot;, subtitle = \u0026quot;Faceted by Region, Sized by Cancer Incidence, Linear Model Fitted by Region\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Facet scatterplot of cancer mortality rate and rate of religious adherence. They were grouped by Region, sized by cancer incidence rate. LM fitted to each group. This plot clarifies slight relationships between religiosity and cancer mortality rates. As expected, these relationships tend to be weak.\n  5. Dimensionality Reduction and Clustering 5a. Running PCA on Dataset set.seed(421) data_pca \u0026lt;-data %\u0026gt;% select_if(is.numeric) %\u0026gt;% scale %\u0026gt;% princomp data_pca_df\u0026lt;- data.frame(PC1=data_pca$scores[, 1], PC2=data_pca$scores[, 2], PC3 = data_pca$scores[, 3]) %\u0026gt;%mutate(name = as.character(data$X1), Region = as.character(data$Region)) eigval \u0026lt;- data_pca$sdev^2 varprop=round(eigval/sum(eigval), 2)  5a. Assess Variance Encompassed by Each Principal Component ggplot() + geom_bar(aes(y=varprop, x=1:6), stat=\u0026quot;identity\u0026quot;) + xlab(\u0026quot;\u0026quot;) + geom_path(aes(y=varprop, x=1:6)) + geom_text(aes(x=1:6, y=varprop, label=round(varprop, 2)), vjust=1, col=\u0026quot;white\u0026quot;, size=5) + scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + scale_x_continuous(breaks=1:10, limit = c(0.5,5.5)) + ggtitle(\u0026quot;Variation Within Each Prinicpal Component\u0026quot;) + ylab(\u0026quot;Variation\u0026quot;) + xlab(\u0026quot;Principal Components\u0026quot;) + theme_minimal() It appears that the first two principal components encompass ~68% of my data’s variation. I can capture up to ~ 86% variation if I visualize my data with the first three principal components.\n 5a. Visualizing Data By Region After PCA fviz_pca_biplot(data_pca, habillage = data_pca_df$Region,label =\u0026quot;var\u0026quot;) + theme_minimal() data_pca_df %\u0026gt;% plot_ly(x=~PC1, y=~PC2, z=~PC3, type=\u0026quot;scatter3d\u0026quot;, mode=\u0026quot;markers\u0026quot;, color = ~Region) %\u0026gt;% layout(scene = list(xaxis = list(title = \u0026#39;PC1 (0.44)\u0026#39;), yaxis = list(title = \u0026#39;PC2 (0.24)\u0026#39;), zaxis = list(title = \u0026#39;PC3 (0.18)\u0026#39;)))  {\"x\":{\"visdat\":{\"68e85488b305\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"68e85488b305\",\"attrs\":{\"68e85488b305\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"markers\",\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"PC1 (0.44)\"},\"yaxis\":{\"title\":\"PC2 (0.24)\"},\"zaxis\":{\"title\":\"PC3 (0.18)\"}},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[0.822306348064632,1.07190906297059,-0.147141037458331,0.199294321369512,-0.5486819703151,0.514633853192868,0.713532673229052,1.37152976013352,0.52640377274337,0.267361548571376,0.944054171291118,0.231128778424715],\"y\":[0.445263093303644,0.187234431605288,0.755372173672134,-0.023831098779958,0.493399345359134,-0.576278812636121,0.69547339300874,-1.80817244966095,-0.493796508091662,0.952726130821671,-0.793837447599091,0.00443657492663399],\"z\":[-0.880936600577315,-0.0176822707947886,0.660594401635903,-0.501516275747051,0.687434704339677,-1.0688829549961,0.455617103016485,-1.75352538943136,-0.99390360205101,1.14931647247675,-1.24707169997073,-0.97388216592004],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"Midwest\",\"marker\":{\"color\":\"rgba(102,194,165,1)\",\"line\":{\"color\":\"rgba(102,194,165,1)\"}},\"textfont\":{\"color\":\"rgba(102,194,165,1)\"},\"error_y\":{\"color\":\"rgba(102,194,165,1)\"},\"error_x\":{\"color\":\"rgba(102,194,165,1)\"},\"line\":{\"color\":\"rgba(102,194,165,1)\"},\"frame\":null},{\"x\":[-0.47822233006605,-0.664721550664497,-0.110640822935078,-0.818694302541133,-2.87978600311518,-2.51086753433792,0.744821107412191,0.334619938495907,1.22504178768941,0.552040137818934,-2.66923993620963],\"y\":[-0.412077651900541,1.44721664016657,-0.549686882245433,0.295456128203461,2.256782135688,0.915242873299899,-0.134875414259115,-0.0326198407507055,0.485101333969798,0.637158564961516,0.932670344671229],\"z\":[-1.61515979102661,-1.07851228349137,-1.81410104491278,0.128354444299685,-0.625537096908529,-1.15961820470845,-0.933292390651044,-0.144887248941461,0.252609340024118,-1.60339094145845,-0.787533889935342],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"Northeast\",\"marker\":{\"color\":\"rgba(252,141,98,1)\",\"line\":{\"color\":\"rgba(252,141,98,1)\"}},\"textfont\":{\"color\":\"rgba(252,141,98,1)\"},\"error_y\":{\"color\":\"rgba(252,141,98,1)\"},\"error_x\":{\"color\":\"rgba(252,141,98,1)\"},\"line\":{\"color\":\"rgba(252,141,98,1)\"},\"frame\":null},{\"x\":[2.94216287467319,1.94743472870561,-1.03793615459785,1.13682464550325,1.88563997508177,2.66113290298741,2.77536371638002,0.844917543663161,1.2269703929894,2.1436573366832,-0.184957946854164,-0.229940207158274],\"y\":[-0.0752557943122514,1.05577150697445,-0.598168804619145,0.22413553987748,2.73995454203208,1.15384861898201,1.30703210104687,0.261123615247009,0.102537708857827,0.613716038538981,-0.373052886393018,2.35693596974787],\"z\":[0.666655165781602,0.191884751196139,1.8314226947022,0.798259695892016,-0.375985124631002,-0.458193452198609,0.263255998415282,1.41760066037626,0.670218397936242,0.979075178367292,1.09151752134985,0.89039417688404],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"Southeast\",\"marker\":{\"color\":\"rgba(141,160,203,1)\",\"line\":{\"color\":\"rgba(141,160,203,1)\"}},\"textfont\":{\"color\":\"rgba(141,160,203,1)\"},\"error_y\":{\"color\":\"rgba(141,160,203,1)\"},\"error_x\":{\"color\":\"rgba(141,160,203,1)\"},\"line\":{\"color\":\"rgba(141,160,203,1)\"},\"frame\":null},{\"x\":[-2.11131140307473,-0.77240247364298,2.02770365941909,1.62545414928106],\"y\":[-1.29076477750668,-2.22836665316224,0.350305139464718,-1.72861031213258],\"z\":[0.866597831676071,0.162891600506687,0.019459868729898,2.9202965961821],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"Southwest\",\"marker\":{\"color\":\"rgba(231,138,195,1)\",\"line\":{\"color\":\"rgba(231,138,195,1)\"}},\"textfont\":{\"color\":\"rgba(231,138,195,1)\"},\"error_y\":{\"color\":\"rgba(231,138,195,1)\"},\"error_x\":{\"color\":\"rgba(231,138,195,1)\"},\"line\":{\"color\":\"rgba(231,138,195,1)\"},\"frame\":null},{\"x\":[-2.29319772221018,-0.528766334892917,-2.33468913286766,-2.08356305271039,-0.350059644500599,-1.84031336754883,-2.44464391177211,-2.48042648733445,2.62393333417113,-2.03618441140133,-1.80348478273613],\"y\":[0.592759393789722,-1.62733269142585,-1.36167153236652,-1.52225493577275,-0.871574691313242,-0.0217521523986969,-0.375858926629271,0.346329375583748,-4.18092893143258,0.414596230007585,-0.941809748419655],\"z\":[0.256445374239515,2.35927179463706,0.280545018368283,-0.433248944726638,-0.655316679849994,-0.304095670007428,0.722945673710484,0.713693792291879,-1.274701741879,0.1869624137359,0.077654794043675],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"West\",\"marker\":{\"color\":\"rgba(166,216,84,1)\",\"line\":{\"color\":\"rgba(166,216,84,1)\"}},\"textfont\":{\"color\":\"rgba(166,216,84,1)\"},\"error_y\":{\"color\":\"rgba(166,216,84,1)\"},\"error_x\":{\"color\":\"rgba(166,216,84,1)\"},\"line\":{\"color\":\"rgba(166,216,84,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} This is the first of three plot pairs. I am showing a biplot and 3D scatterplot of the scaled data (colored by Region).\nFrom the biplot, we can ascertain that PC1 is positively related to every numeric variable in the dataset. PC2 is strongly positively associated with mortality and incidence rates but is somewhat negatively related to rates_of_adherence and total number of congregations.\nI included the 3D scatterplot to help understand the data. While two principal components are sufficient to glean ~70% of the data’s variation, the addition of a third principal component yields about ~87% of the variation.\nAs for the distribution of the points by Region, some regions are very well defined in the feature space. The West and Southeast, for instance, are well separated from the other points; Midwest, Northeast, and Southwest are less well-defined.\n 5b. Validate Optimal K for Kmeans max_k = 11 wss\u0026lt;-vector() sil_width\u0026lt;-vector() for(i in 2:max_k){ temp\u0026lt;- data %\u0026gt;% select_if(is.numeric) %\u0026gt;% scale %\u0026gt;% kmeans(i) wss[i]\u0026lt;-temp$tot.withinss sil \u0026lt;- silhouette(temp$cluster,dist(data)) sil_width[i]\u0026lt;-mean(sil[,3]) } ggplot()+geom_point(aes(x=1:max_k,y=wss))+geom_path(aes(x=1:max_k,y=wss))+ xlab(\u0026quot;clusters\u0026quot;)+scale_x_continuous(breaks=1:max_k) + xlim(2,max_k) + xlim(2,max_k) + xlab(\u0026quot;K\u0026quot;) + ylab(\u0026quot;WSS\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;WSS Silhouette Width by K\u0026quot;) ## Scale for \u0026#39;x\u0026#39; is already present. Adding another scale for \u0026#39;x\u0026#39;, which will ## replace the existing scale. ## Scale for \u0026#39;x\u0026#39; is already present. Adding another scale for \u0026#39;x\u0026#39;, which will ## replace the existing scale. ggplot()+geom_line(aes(x=1:max_k,y=sil_width))+scale_x_continuous(name=\u0026quot;k\u0026quot;,breaks=1:max_k)+ xlim(2,max_k) + xlab(\u0026quot;K\u0026quot;) + ylab(\u0026quot;Mean Silhouette Width\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;Mean Silhouette Width by K\u0026quot;) ## Scale for \u0026#39;x\u0026#39; is already present. Adding another scale for \u0026#39;x\u0026#39;, which will ## replace the existing scale. Based on the average silhouette width plot and the WSS plot, the optimal value of K is 2. This value of K corresponds to an average silhouette width of ~0.11 and a WSS of ~200.\n 5b. Assess Cluster “Goodness” with (Mesa-themed) Silhouette Plot km \u0026lt;- data %\u0026gt;% select_if(is.numeric) %\u0026gt;%scale %\u0026gt;% kmeans(2) ss \u0026lt;- silhouette(km$cluster, dist(data%\u0026gt;% select_if(is.numeric))) plot(ss,col = c(\u0026#39;#E53D57\u0026#39;, \u0026quot;#E68E0C\u0026quot;), border = NA, main = \u0026quot;Silhouette Plot of Kmeans Clusters\u0026quot;) As expected, the clusters look absolutely wretched. Average silhouette width is 0.11.\n 5b. Visualizing Kmeans Clusters In Feature Space (with Principal Components) data_pca_df_km \u0026lt;- data_pca_df %\u0026gt;% mutate(cluster=as.factor(km$cluster)) fviz_pca_biplot(data_pca, habillage = data_pca_df_km$cluster,label =\u0026quot;var\u0026quot;) + theme_minimal() data_pca_df_km %\u0026gt;% plot_ly(x=~PC1, y=~PC2, z=~PC3, type=\u0026quot;scatter3d\u0026quot;, mode=\u0026quot;markers\u0026quot;, color = ~cluster) %\u0026gt;% layout(scene = list(xaxis = list(title = \u0026#39;PC1 (0.44)\u0026#39;), yaxis = list(title = \u0026#39;PC2 (0.24)\u0026#39;), zaxis = list(title = \u0026#39;PC3 (0.18)\u0026#39;)))  {\"x\":{\"visdat\":{\"68e822201d46\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"68e822201d46\",\"attrs\":{\"68e822201d46\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"markers\",\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"PC1 (0.44)\"},\"yaxis\":{\"title\":\"PC2 (0.24)\"},\"zaxis\":{\"title\":\"PC3 (0.18)\"}},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[-2.29319772221018,-2.11131140307473,-0.528766334892917,-2.33468913286766,-1.03793615459785,-2.08356305271039,-0.350059644500599,-0.818694302541133,-2.87978600311518,-0.5486819703151,-1.84031336754883,-2.51086753433792,-0.77240247364298,-2.44464391177211,-2.48042648733445,-0.184957946854164,-2.66923993620963,-2.03618441140133,-1.80348478273613],\"y\":[0.592759393789722,-1.29076477750668,-1.62733269142585,-1.36167153236652,-0.598168804619145,-1.52225493577275,-0.871574691313242,0.295456128203461,2.256782135688,0.493399345359134,-0.0217521523986969,0.915242873299899,-2.22836665316224,-0.375858926629271,0.346329375583748,-0.373052886393018,0.932670344671229,0.414596230007585,-0.941809748419655],\"z\":[0.256445374239515,0.866597831676071,2.35927179463706,0.280545018368283,1.8314226947022,-0.433248944726638,-0.655316679849994,0.128354444299685,-0.625537096908529,0.687434704339677,-0.304095670007428,-1.15961820470845,0.162891600506687,0.722945673710484,0.713693792291879,1.09151752134985,-0.787533889935342,0.1869624137359,0.077654794043675],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"1\",\"marker\":{\"color\":\"rgba(102,194,165,1)\",\"line\":{\"color\":\"rgba(102,194,165,1)\"}},\"textfont\":{\"color\":\"rgba(102,194,165,1)\"},\"error_y\":{\"color\":\"rgba(102,194,165,1)\"},\"error_x\":{\"color\":\"rgba(102,194,165,1)\"},\"line\":{\"color\":\"rgba(102,194,165,1)\"},\"frame\":null},{\"x\":[2.94216287467319,1.94743472870561,-0.47822233006605,-0.664721550664497,1.13682464550325,0.822306348064632,1.07190906297059,-0.147141037458331,0.199294321369512,1.88563997508177,2.66113290298741,-0.110640822935078,0.514633853192868,0.713532673229052,2.77536371638002,0.844917543663161,1.37152976013352,0.52640377274337,0.744821107412191,0.334619938495907,0.267361548571376,2.02770365941909,1.22504178768941,0.552040137818934,1.2269703929894,0.944054171291118,2.1436573366832,1.62545414928106,2.62393333417113,0.231128778424715,-0.229940207158274],\"y\":[-0.0752557943122514,1.05577150697445,-0.412077651900541,1.44721664016657,0.22413553987748,0.445263093303644,0.187234431605288,0.755372173672134,-0.023831098779958,2.73995454203208,1.15384861898201,-0.549686882245433,-0.576278812636121,0.69547339300874,1.30703210104687,0.261123615247009,-1.80817244966095,-0.493796508091662,-0.134875414259115,-0.0326198407507055,0.952726130821671,0.350305139464718,0.485101333969798,0.637158564961516,0.102537708857827,-0.793837447599091,0.613716038538981,-1.72861031213258,-4.18092893143258,0.00443657492663399,2.35693596974787],\"z\":[0.666655165781602,0.191884751196139,-1.61515979102661,-1.07851228349137,0.798259695892016,-0.880936600577315,-0.0176822707947886,0.660594401635903,-0.501516275747051,-0.375985124631002,-0.458193452198609,-1.81410104491278,-1.0688829549961,0.455617103016485,0.263255998415282,1.41760066037626,-1.75352538943136,-0.99390360205101,-0.933292390651044,-0.144887248941461,1.14931647247675,0.019459868729898,0.252609340024118,-1.60339094145845,0.670218397936242,-1.24707169997073,0.979075178367292,2.9202965961821,-1.274701741879,-0.97388216592004,0.89039417688404],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"2\",\"marker\":{\"color\":\"rgba(141,160,203,1)\",\"line\":{\"color\":\"rgba(141,160,203,1)\"}},\"textfont\":{\"color\":\"rgba(141,160,203,1)\"},\"error_y\":{\"color\":\"rgba(141,160,203,1)\"},\"error_x\":{\"color\":\"rgba(141,160,203,1)\"},\"line\":{\"color\":\"rgba(141,160,203,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} These are the same plots as previously shown; however, marker colors were changed to represent clusters defined by Kmeans. Clusters are somewhat prominent; Cluster 1 is high on PC1, Cluster 2 is low on PC1.\n 5c. Validate Optimal K for PAM max_k = 10 pam_dat\u0026lt;-data %\u0026gt;% select_if(is.numeric) %\u0026gt;% scale sil_width\u0026lt;-vector() for(i in 2:max_k){ pam_fit \u0026lt;- pam(pam_dat, k = i) sil_width[i] \u0026lt;- pam_fit$silinfo$avg.width } ggplot()+geom_line(aes(x=1:max_k,y=sil_width))+scale_x_continuous(name=\u0026quot;k\u0026quot;,breaks=1:max_k) + xlim(2,max_k) + xlab(\u0026quot;K\u0026quot;) + ylab(\u0026quot;Mean Silhouette Width\u0026quot;) + theme_minimal() + ggtitle(\u0026quot;Mean Silhouette Width by K\u0026quot;) ## Scale for \u0026#39;x\u0026#39; is already present. Adding another scale for \u0026#39;x\u0026#39;, which will ## replace the existing scale. Even though k=6 has a higher silhouette score than k=4, I choose to set k=4. This is one of those interpretability-tightness tradeoffs.\n 5c. Assess PAM Cluster “Goodness” with another (Mesa-themed) Silhouette Plot gower1 \u0026lt;-daisy(data %\u0026gt;% select(-X1) %\u0026gt;% mutate_if(is.character,as.factor) %\u0026gt;% select_if(is.numeric) %\u0026gt;% scale,metric=\u0026quot;gower\u0026quot;) pam_fit \u0026lt;- pam(gower1,k = 4,diss =T) plot(silhouette(pam_fit$clustering,gower1), col = c(\u0026#39;#E53D57\u0026#39;,\u0026quot;#D9563A\u0026quot;, \u0026quot;#E68E0C\u0026quot;, \u0026quot;#89669D\u0026quot;), border = NA,main =\u0026quot;Silhouette Plot of PAM Clusters\u0026quot;) It’s clear that with PAM, my data clusters a little more cleanly. The clusters still aren’t great (as seen by the very small 0.26 mean silhouette width). Cluster two is fairly tight, and cluster four is somewhat diffuse. Each cluster has one or two samples that don’t belong.\n 5c. Visualizing Kmeans Clusters In Feature Space (with Principal Components) data_pca_df_pam \u0026lt;- data_pca_df %\u0026gt;% mutate(cluster=as.factor(pam_fit$clustering)) fviz_pca_biplot(data_pca, habillage = data_pca_df_pam$cluster, label =\u0026quot;var\u0026quot;) data_pca_df_pam %\u0026gt;% plot_ly(x=~PC1, y=~PC2, z=~PC3, type=\u0026quot;scatter3d\u0026quot;, mode=\u0026quot;markers\u0026quot;, color = ~cluster) %\u0026gt;% layout(scene = list(xaxis = list(title = \u0026#39;PC1 (0.44)\u0026#39;), yaxis = list(title = \u0026#39;PC2 (0.24)\u0026#39;), zaxis = list(title = \u0026#39;PC3 (0.18)\u0026#39;)))  {\"x\":{\"visdat\":{\"68e838c8e767\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"68e838c8e767\",\"attrs\":{\"68e838c8e767\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"markers\",\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"PC1 (0.44)\"},\"yaxis\":{\"title\":\"PC2 (0.24)\"},\"zaxis\":{\"title\":\"PC3 (0.18)\"}},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[-2.29319772221018,-2.11131140307473,-2.33468913286766,-2.08356305271039,-2.87978600311518,-1.84031336754883,-2.51086753433792,-0.77240247364298,-2.44464391177211,-2.48042648733445,-2.66923993620963,-2.03618441140133,-1.80348478273613],\"y\":[0.592759393789722,-1.29076477750668,-1.36167153236652,-1.52225493577275,2.256782135688,-0.0217521523986969,0.915242873299899,-2.22836665316224,-0.375858926629271,0.346329375583748,0.932670344671229,0.414596230007585,-0.941809748419655],\"z\":[0.256445374239515,0.866597831676071,0.280545018368283,-0.433248944726638,-0.625537096908529,-0.304095670007428,-1.15961820470845,0.162891600506687,0.722945673710484,0.713693792291879,-0.787533889935342,0.1869624137359,0.077654794043675],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"1\",\"marker\":{\"color\":\"rgba(102,194,165,1)\",\"line\":{\"color\":\"rgba(102,194,165,1)\"}},\"textfont\":{\"color\":\"rgba(102,194,165,1)\"},\"error_y\":{\"color\":\"rgba(102,194,165,1)\"},\"error_x\":{\"color\":\"rgba(102,194,165,1)\"},\"line\":{\"color\":\"rgba(102,194,165,1)\"},\"frame\":null},{\"x\":[2.94216287467319,1.94743472870561,1.13682464550325,1.88563997508177,2.66113290298741,2.77536371638002,2.02770365941909,1.2269703929894,2.1436573366832,-0.229940207158274],\"y\":[-0.0752557943122514,1.05577150697445,0.22413553987748,2.73995454203208,1.15384861898201,1.30703210104687,0.350305139464718,0.102537708857827,0.613716038538981,2.35693596974787],\"z\":[0.666655165781602,0.191884751196139,0.798259695892016,-0.375985124631002,-0.458193452198609,0.263255998415282,0.019459868729898,0.670218397936242,0.979075178367292,0.89039417688404],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"2\",\"marker\":{\"color\":\"rgba(252,141,98,1)\",\"line\":{\"color\":\"rgba(252,141,98,1)\"}},\"textfont\":{\"color\":\"rgba(252,141,98,1)\"},\"error_y\":{\"color\":\"rgba(252,141,98,1)\"},\"error_x\":{\"color\":\"rgba(252,141,98,1)\"},\"line\":{\"color\":\"rgba(252,141,98,1)\"},\"frame\":null},{\"x\":[-0.528766334892917,-0.664721550664497,-1.03793615459785,-0.147141037458331,-0.818694302541133,-0.5486819703151,0.713532673229052,0.844917543663161,0.267361548571376,-0.184957946854164],\"y\":[-1.62733269142585,1.44721664016657,-0.598168804619145,0.755372173672134,0.295456128203461,0.493399345359134,0.69547339300874,0.261123615247009,0.952726130821671,-0.373052886393018],\"z\":[2.35927179463706,-1.07851228349137,1.8314226947022,0.660594401635903,0.128354444299685,0.687434704339677,0.455617103016485,1.41760066037626,1.14931647247675,1.09151752134985],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"3\",\"marker\":{\"color\":\"rgba(141,160,203,1)\",\"line\":{\"color\":\"rgba(141,160,203,1)\"}},\"textfont\":{\"color\":\"rgba(141,160,203,1)\"},\"error_y\":{\"color\":\"rgba(141,160,203,1)\"},\"error_x\":{\"color\":\"rgba(141,160,203,1)\"},\"line\":{\"color\":\"rgba(141,160,203,1)\"},\"frame\":null},{\"x\":[-0.47822233006605,0.822306348064632,-0.350059644500599,1.07190906297059,0.199294321369512,-0.110640822935078,0.514633853192868,1.37152976013352,0.52640377274337,0.744821107412191,0.334619938495907,1.22504178768941,0.552040137818934,0.944054171291118,1.62545414928106,2.62393333417113,0.231128778424715],\"y\":[-0.412077651900541,0.445263093303644,-0.871574691313242,0.187234431605288,-0.023831098779958,-0.549686882245433,-0.576278812636121,-1.80817244966095,-0.493796508091662,-0.134875414259115,-0.0326198407507055,0.485101333969798,0.637158564961516,-0.793837447599091,-1.72861031213258,-4.18092893143258,0.00443657492663399],\"z\":[-1.61515979102661,-0.880936600577315,-0.655316679849994,-0.0176822707947886,-0.501516275747051,-1.81410104491278,-1.0688829549961,-1.75352538943136,-0.99390360205101,-0.933292390651044,-0.144887248941461,0.252609340024118,-1.60339094145845,-1.24707169997073,2.9202965961821,-1.274701741879,-0.97388216592004],\"mode\":\"markers\",\"type\":\"scatter3d\",\"name\":\"4\",\"marker\":{\"color\":\"rgba(231,138,195,1)\",\"line\":{\"color\":\"rgba(231,138,195,1)\"}},\"textfont\":{\"color\":\"rgba(231,138,195,1)\"},\"error_y\":{\"color\":\"rgba(231,138,195,1)\"},\"error_x\":{\"color\":\"rgba(231,138,195,1)\"},\"line\":{\"color\":\"rgba(231,138,195,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} These principal components are the same as in the previous plot. The only element changed is the marker color, which now represents clusters determined via PAM.\nCluster 1 is very low on PC1 and nearly 0 on PC2. Cluster 2 is very high on PC1 and almost 0 on PC2. It is impossible to differentiate clusters 3 and 4 using only two principal components. These each score near 0 on PC1 and PC2, but Cluster 3 scores slightly positive on PC3 while Cluster 4 scores slightly negative on PC3.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a98abdade45b245ad23f89a6882c361","permalink":"/projects/project1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/project1/","section":"projects","summary":"EID: REB3566  0. Introduction Three datasets are combined and explored in this R markdown file. The first dataset contains cancer mortality and incidence by states in the United States.","tags":null,"title":"Exploratory Data Analysis in R","type":"projects"}]