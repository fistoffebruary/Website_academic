<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Ryan Bailey</title>
    <link>/projects/</link>
      <atom:link href="/projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 26 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/projects/</link>
    </image>
    
    <item>
      <title>Fitting Models on High Dimensional Biological Data (in R)</title>
      <link>/projects/project2/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/projects/project2/</guid>
      <description>


&lt;div id=&#34;reb3566&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;REB3566&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;welcome-to-for-loop-hll&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Welcome to for loop h*ll&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;0a. Introduction&lt;/h1&gt;
&lt;p&gt;This analysis is based on The Cancer Genome Atlas’ Pancreatic Adenocarcinoma Project (TCGA-PAAD). TCGA provides large, well-documented cancer datasets that are semi open-source. Three datasets, HT-Seq FPKM, survival data, and phenotype data, were acquired from UCSC Xena, a data download portal from UC Santa Cruz &lt;a href=&#34;https://xenabrowser.net/datapages/?cohort=GDC%20TCGA%20Pancreatic%20Cancer%20(PAAD)&amp;amp;removeHub=https%3A%2F%2Fxena.treehouse.gi.ucsc.edu%3A443&#34;&gt;UCSC Xena&lt;/a&gt;. There are 182 complete cases for these datasets. The HT-Seq dataset has 60,483 variables, all of which are numeric. These are measured in &lt;em&gt;log2(fpkm+1) or log2(Fragments Per Kilobase of transcript per Million mapped reads + 1)&lt;/em&gt;. In RNA-Seq, the relative expression of a transcript is proportional to the number of cDNA fragments that originate from it. The survival data contains two variables, days_survived (OS.time) and OS (Overall Survival), where &lt;strong&gt;1&lt;/strong&gt; is a survival event (death), and &lt;strong&gt;0&lt;/strong&gt; is no survival event. The phenotype data contains 122 variables encompassing treatment information, tumor grading/staging, patient health behaviors, and demographic information.&lt;/p&gt;
&lt;p&gt;One of the key categorical variables that I come back to frequently is &lt;strong&gt;organ of origin.&lt;/strong&gt; When I binarize this variable, I encode &lt;strong&gt;Head of Pancreas&lt;/strong&gt; as &lt;strong&gt;1&lt;/strong&gt; and all other locations as &lt;strong&gt;0.&lt;/strong&gt; &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2575681/&#34;&gt;For context, pancreatic cancers that originate in the head of the pancreas often have better prognoses than those that arise in other areas.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: I plan on doing inline comments on my code, but will also narrate my results, interpretations, and thought processes after each code chunk.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-importing-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;0b. Importing Libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for reading in my datasets
library(readr)

# for data wrangling, cleaning, etc.
library(tidyverse)

# plotting
library(ggplot2)

# Heteroscedasticity-Consistent Covariance Matrix
# Estimation
library(sandwich)

# test for heteroskedasticity
library(lmtest)

# ROC plots
library(plotROC)

# LASSO
library(glmnet)


# Things to build a hierarchically clustered
# heatmap
library(prettyR)
library(reshape2)
library(ComplexHeatmap)
library(circlize)

# for PCA Biplot
library(factoextra)

# for plotting interactions
library(interactions)

# manova assumptions
library(rstatix)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;c-g-github-didnt-like-my-130-mb-dataset-so-i-just-created-a-.csv-with-my-edaedsubset-data-and-import-that&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;0c-g: Github didn’t like my 130 Mb dataset so I just created a .csv with my EDAed/subset data and import that&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;quot;Project2_data.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;h.-pca&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;0h. PCA&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

data_pca &amp;lt;- data %&amp;gt;% select(contains(&amp;quot;ENSG&amp;quot;)) %&amp;gt;% scale %&amp;gt;% 
    princomp
data_pca_df &amp;lt;- data.frame(PC1 = data_pca$scores[, 1], 
    PC2 = data_pca$scores[, 2], PC3 = data_pca$scores[, 
        3], HoP = as.factor(data$Head_of_pancreas))

ggplot(data_pca_df, aes(PC1, PC2, color = HoP)) + geom_point() + 
    theme_minimal() + geom_point(data = data_pca_df %&amp;gt;% 
    group_by(HoP) %&amp;gt;% summarize(x = mean(PC2), y = mean(PC2)), 
    aes(x, y), size = 6, shape = 3) + ggtitle(&amp;quot;PCA of Data Colored By Head of Pancreas&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/PCA-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It became instantly obvious to me that MANOVA and all of the hypothesis tests for its assumptions break down with high dimensional data. I’m not sure why this is. Rather than hand picking or randomly picking a subset of variables, I perform PCA on the data and work with principal components instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-assessing-manova-assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1a. Assessing MANOVA Assumptions&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group &amp;lt;- as.character(data_pca_df$HoP)
DVs &amp;lt;- data_pca_df %&amp;gt;% select(PC1, PC2, PC3)

# Test multivariate normality for each group (null:
# assumption met)
sapply(split(DVs, group), mshapiro_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           0           1           
## statistic 0.9147001   0.9350529   
## p.value   0.001200223 9.521684e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If any p&amp;lt;.05, stop (assumption violated). If not,
# test homogeneity of covariance matrices

# Box&amp;#39;s M test (null: assumption met)

# box_m(DVs, group)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Welp, it appears I violate some of the assumptions of the MANOVA, but that’s no surprise. The samples are not likely to be fully random. PCA1, PC2, and PC3 are neither univariately normal nor multivariately normal. There is not a strong linear relationship between PC1, PC2, PC3 There are several groups of outliers for the variables.&lt;/p&gt;
&lt;p&gt;Box’s test was not performed because the data did not meet the multivariate normality assumption.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-manova-head-of-pancreas-and-principal-components&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1b. MANOVA: Head of Pancreas and Principal Components&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;man &amp;lt;- manova(cbind(PC1, PC2, PC3) ~ HoP, data = data_pca_df)
summary(man, tol = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Df  Pillai approx F num Df den Df   Pr(&amp;gt;F)    
## HoP         1 0.10262   6.7852      3    178 0.000234 ***
## Residuals 180                                            
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary.aov(man)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Response PC1 :
##              Df Sum Sq Mean Sq F value   Pr(&amp;gt;F)   
## HoP           1  209.8 209.843   8.114 0.004904 **
## Residuals   180 4655.1  25.862                    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
##  Response PC2 :
##              Df  Sum Sq Mean Sq F value  Pr(&amp;gt;F)  
## HoP           1   41.66  41.659  5.5778 0.01926 *
## Residuals   180 1344.37   7.469                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
##  Response PC3 :
##              Df Sum Sq Mean Sq F value  Pr(&amp;gt;F)  
## HoP           1  24.24 24.2394  5.4584 0.02058 *
## Residuals   180 799.34  4.4408                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test((data_pca_df$PC1), data_pca_df$HoP, 
    p.adj = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  (data_pca_df$PC1) and data_pca_df$HoP 
## 
##   0     
## 1 0.0049
## 
## P value adjustment method: none&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test((data_pca_df$PC2), data_pca_df$HoP, 
    p.adj = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  (data_pca_df$PC2) and data_pca_df$HoP 
## 
##   0    
## 1 0.019
## 
## P value adjustment method: none&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test((data_pca_df$PC3), data_pca_df$HoP, 
    p.adj = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  (data_pca_df$PC3) and data_pca_df$HoP 
## 
##   0    
## 1 0.021
## 
## P value adjustment method: none&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bonf_p &amp;lt;- 0.05/7
bonf_p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.007142857&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob_typeI &amp;lt;- 1 - (0.95^7)
prob_typeI&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3016627&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Significant differences were found between head of pancreas samples and non-head of pancreas samples. After performing 7 hypothesis tests, the probablility of a Type I error was 0.3016627. The bonferroni-corrected p-value is 0.007142857. While all hypothesis tests returned p-values less than 0.05. Only the MANOVA, PC1 ANOVA, and the PC1 pairwise t-test returned p-values less than the bonf_p. This means that only head of pancreas and non-head of pancreas only significantly different for principal component 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-randomization-tests-mean-difference-in-pc-values-in-hop-1-vs-hop-0&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2a. Randomization Tests: Mean Difference in PC Values in HoP = 1 vs HoP = 0&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the seed
set.seed(1)


# randomization_test takes a data.frame, a column
# name (&amp;#39;string&amp;#39;), and an optional iters argument
# it returns a histogram of bootstrapped mean
# distances.

randomization_test &amp;lt;- function(data, column, iters = 5000) {
    rand_dist &amp;lt;- c()
    new &amp;lt;- data %&amp;gt;% select(column, HoP)
    for (i in 1:iters) {
        temp &amp;lt;- new[sample(1:nrow(new), size = 10, 
            replace = T), ] %&amp;gt;% group_by(HoP) %&amp;gt;% summarize_if(is.numeric, 
            mean)
        if (nrow(temp) == 1) {
            rand_dist[i] &amp;lt;- temp %&amp;gt;% pull(column)
        }
        if (nrow(temp) == 2) {
            rand_dist[i] &amp;lt;- temp %&amp;gt;% summarize_if(is.numeric, 
                diff) %&amp;gt;% pull(column)
        }
    }
    # get the lower and upper bounds of all the
    # mean_dists for that gene
    lb &amp;lt;- quantile(rand_dist, 0.025)
    ub &amp;lt;- quantile(rand_dist, 0.975)
    
    
    data.frame(means = rand_dist) %&amp;gt;% ggplot(aes(x = means)) + 
        geom_histogram(alpha = 0.75, fill = &amp;quot;gray&amp;quot;) + 
        theme_minimal() + geom_vline(xintercept = mean(rand_dist), 
        color = &amp;quot;red&amp;quot;) + geom_vline(xintercept = lb, 
        color = &amp;quot;green&amp;quot;) + geom_vline(xintercept = ub, 
        color = &amp;quot;green&amp;quot;) + ggtitle(paste(i, &amp;quot;Bootstrapped Mean Differences Between HoP = 1 vs HoP = 0 for&amp;quot;, 
        column)) + xlab(&amp;quot;Differences in Means&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Run%20Randomization%20Tests-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Run%20Randomization%20Tests-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Run%20Randomization%20Tests-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The null hypothesis for each of the three hypothesis tests is that the HoP = 1 group and HoP = 0 group do not differ in their means.&lt;/p&gt;
&lt;p&gt;The alternative hypothesis for each of the three hypothesis tests is that the HoP = 1 group and HoP = 0 group do differ in their means.&lt;/p&gt;
&lt;p&gt;For context, the red line in each histogram is the mean, and the green lines are the 95 CI. It’s pretty clear to me that in each of the three plots, the 95 CI encompasses 0.&lt;/p&gt;
&lt;p&gt;The mean distributions between Head of pancreas (HoP = 1) and non-Head of pancreas (HoP = 0) samples was not significantly different for PC1, PC2, or PC3. This is because for each variable, the 95 CIs of mean differences encompass 0. I cannot reject the null hypothesis for any of these variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-lm-predicting-survival-time-from-principal-components&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3a. LM: Predicting Survival Time from Principal Components&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create mean centered principal component
# variables; rename OS.time to `y`
data_pca_df &amp;lt;- data_pca_df %&amp;gt;% mutate(PC1_c = PC1 - 
    mean(PC1), PC2_c = PC2 - mean(PC2)) %&amp;gt;% mutate(y = data$OS.time)


fit &amp;lt;- lm(y ~ PC1_c * PC2_c, data = data_pca_df)

# check normality
qqnorm(fit$residuals, main = &amp;quot;QQ-plot of Model Residuals&amp;quot;)
qqline(fit$residuals, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/LM%20PCs-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check for uniform variance; check linearity
res &amp;lt;- data.frame(fitted.values = fit$fitted.values, 
    residuals = fit$residuals)
res %&amp;gt;% ggplot(aes(fitted.values, residuals)) + geom_smooth(color = &amp;quot;black&amp;quot;, 
    se = T) + geom_point() + geom_hline(aes(yintercept = 0), 
    color = &amp;quot;red&amp;quot;) + theme_minimal() + ggtitle(&amp;quot;Plotting Residuals by Fitted Values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/LM%20PCs-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hypothesis test for uniform variance
bptest(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 4.4287, df = 3, p-value = 0.2187&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# model summary
summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ PC1_c * PC2_c, data = data_pca_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -680.91 -290.30  -93.65  143.17 1865.01 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  564.670     34.065  16.576   &amp;lt;2e-16 ***
## PC1_c         -6.185      6.845  -0.904   0.3674    
## PC2_c        -31.236     13.778  -2.267   0.0246 *  
## PC1_c:PC2_c    4.184      2.716   1.540   0.1253    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 458.3 on 177 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.07875,    Adjusted R-squared:  0.06313 
## F-statistic: 5.043 on 3 and 177 DF,  p-value: 0.002236&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# robust SEs model summary
coeftest(fit, vcov = vcovHC(fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 564.6699    34.5618 16.3380  &amp;lt; 2e-16 ***
## PC1_c        -6.1855     6.0165 -1.0281  0.30531    
## PC2_c       -31.2358    18.7447 -1.6664  0.09741 .  
## PC1_c:PC2_c   4.1836     2.9188  1.4333  0.15353    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# interaction plot for PC1_c, PC2_c
interact_plot(fit, pred = PC1_c, modx = PC2_c, data = data_pca_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/LM%20PCs-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OS.time =564.67 - 6.185(PC1_c) - 31.236(PC2_c) + 4.184(PC1_c * PC2_c)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It’s clear from the QQ plot that my data is not normal at all. That said, the BP Test and even distribution of residuals around 0 prove show that the data is homoskedastic and exhibits a linear trend.&lt;/p&gt;
&lt;p&gt;For samples of average PC1 and PC2, the predicted overall survival time is 564.67 days. PC1_c has a coefficient of -6.185. This means that for every one-unit increase in PC1_c, OS.time decreases by 6.185, on average. (not significant) PC2_c has a coefficient of -31.236. This means that for every one-unit increase in PC2_c, OS.time decreases by 31.236, on average. (significant) The coefficent for PC1_c:PC2_c is 4.184. This shows that as PC2_c increases, the effect of PC1_c on OS.time becomes more positive. (not significant)&lt;/p&gt;
&lt;p&gt;This model has an adjusted R-squared of 0.06313; this means that my model explains 6.313% of the variation in OS.time&lt;/p&gt;
&lt;p&gt;The Robust SE LM returned no significant coefficients save for the intercept. This means that because Robust SEs were used, PC2_c is no longer a significant predictor of Head of Pancreas.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-bootstrapped-se-lm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4a. Bootstrapped SE LM&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

samp_distn &amp;lt;- replicate(5000, {
    boot_dat &amp;lt;- sample_frac(data_pca_df, replace = T)
    
    fit &amp;lt;- lm(y ~ PC1_c * PC2_c, data = boot_dat)
    coef(fit)
})

samp_distn %&amp;gt;% t %&amp;gt;% as.data.frame %&amp;gt;% summarize_all(sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)    PC1_c    PC2_c PC1_c:PC2_c
## 1    33.60516 6.041935 18.34254    2.882069&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;samp_distn %&amp;gt;% t %&amp;gt;% as.data.frame %&amp;gt;% pivot_longer(everything()) %&amp;gt;% 
    group_by(name) %&amp;gt;% summarize(lower = quantile(value, 
    0.025), upper = quantile(value, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 3
##   name         lower  upper
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 (Intercept) 500.   631.  
## 2 PC1_c       -17.2    6.12
## 3 PC1_c:PC2_c  -1.79   9.72
## 4 PC2_c       -66.0    5.87&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The intercept’s 95 CI does not cross 0 and is significant. All other 95 CIs include 0 and indicate that the other variables are not significant predictors. This yielded the same results as the Robust SEs LM.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-define-class_diag-and-conf_matrix-functions-with-optional-decision-threshold-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5a. Define class_diag and conf_matrix functions with optional decision threshold parameters&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class_diag &amp;lt;- function(probs, truth, thresh = 0.5) {
    tab &amp;lt;- table(factor(probs &amp;gt; thresh, levels = c(&amp;quot;FALSE&amp;quot;, 
        &amp;quot;TRUE&amp;quot;)), truth)
    acc = sum(diag(tab))/sum(tab)
    sens = tab[2, 2]/colSums(tab)[2]
    spec = tab[1, 1]/colSums(tab)[1]
    ppv = tab[2, 2]/rowSums(tab)[2]
    f1 = 2 * (sens * ppv)/(sens + ppv)
    
    if (is.numeric(truth) == FALSE &amp;amp; is.logical(truth) == 
        FALSE) {
        truth &amp;lt;- as.numeric(truth) - 1
    }
    
    # CALCULATE EXACT AUC
    ord &amp;lt;- order(probs, decreasing = TRUE)
    probs &amp;lt;- probs[ord]
    truth &amp;lt;- truth[ord]
    
    TPR = cumsum(truth)/max(1, sum(truth))
    FPR = cumsum(!truth)/max(1, sum(!truth))
    
    dup &amp;lt;- c(probs[-1] &amp;gt;= probs[-length(probs)], FALSE)
    TPR &amp;lt;- c(0, TPR[!dup], 1)
    FPR &amp;lt;- c(0, FPR[!dup], 1)
    
    n &amp;lt;- length(TPR)
    auc &amp;lt;- sum(((TPR[-1] + TPR[-n])/2) * (FPR[-1] - 
        FPR[-n]))
    
    data.frame(acc, sens, spec, ppv, f1, auc)
}
# prints a confusion matrix to the screen
conf_matrix &amp;lt;- function(probs, truth, thresh = 0.5) {
    table(factor(probs &amp;gt; thresh, levels = c(&amp;quot;FALSE&amp;quot;, 
        &amp;quot;TRUE&amp;quot;)), truth)
}

# find optimal thresh optimizes the decision
# threshold to yield the maximum f1 score
find_optimal_thresh &amp;lt;- function(fit, data_genes, probs, 
    plot = FALSE) {
    data_genes$probs &amp;lt;- probs
    # initialize f_df variable
    f_df &amp;lt;- NULL
    # for i in 1000 iterations, get the f1-score for
    # every possible cutoff between 0 and 1,
    # incrementing 0.001 each iteration
    for (i in 1:1000) {
        f_df &amp;lt;- rbind(f_df, data.frame(cutoff = i/1000, 
            f1 = class_diag(data_genes$probs, data_genes$Head_of_pancreas, 
                i/1000)$f1))
    }
    
    # get decision threshold that yielded the highest
    # F1-score
    thresh &amp;lt;- (f_df %&amp;gt;% arrange(desc(f1)))[1, ] %&amp;gt;% 
        pull(cutoff)
    if (plot == TRUE) {
        print(ggplot(f_df, aes(cutoff, f1)) + geom_line() + 
            geom_vline(aes(xintercept = thresh)) + 
            xlab(&amp;quot;Decision Threshold&amp;quot;) + ylab(&amp;quot;F1-score&amp;quot;) + 
            theme_minimal() + ggtitle(&amp;quot;Identifying the Optimal Decision Threshold&amp;quot;))
    }
    
    
    return(thresh)
}

# generate density plot separated by
# head_of_pancreas group
logit_density &amp;lt;- function(fit, Head_of_pancreas) {
    # plot frequency plot of logit for both classes
    print(data.frame(predict = predict(fit, type = &amp;quot;link&amp;quot;), 
        Head_of_pancreas = Head_of_pancreas) %&amp;gt;% ggplot(aes(predict)) + 
        geom_density(aes(fill = Head_of_pancreas), 
            alpha = 0.5) + theme_minimal() + geom_vline(aes(xintercept = 0)) + 
        xlab(&amp;quot;logit&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-logistic-regression-predicting-head-of-pancreas-from-three-randomly-picked-genes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5b. Logistic Regression Predicting Head of Pancreas from Three Randomly Picked Genes&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(5)
data_genes &amp;lt;- data %&amp;gt;% select(Head_of_pancreas, contains(&amp;quot;ENSG&amp;quot;))
data_genes &amp;lt;- data_genes %&amp;gt;% select(sample(2:ncol(data_genes), 
    3), Head_of_pancreas)
fit &amp;lt;- glm(Head_of_pancreas ~ ., family = &amp;quot;binomial&amp;quot;, 
    data = data_genes, control = list(maxit = 75))

# get class probabilities
data_genes$probs &amp;lt;- predict(fit, type = &amp;quot;response&amp;quot;)
summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Head_of_pancreas ~ ., family = &amp;quot;binomial&amp;quot;, data = data_genes, 
##     control = list(maxit = 75))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9616  -1.3477   0.6723   0.9301   1.0202  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)         0.39463    0.24572   1.606    0.108
## ENSG00000169347.15  0.07724    0.13142   0.588    0.557
## ENSG00000179751.6   0.17209    0.28277   0.609    0.543
## ENSG00000142615.7  -0.10031    0.25401  -0.395    0.693
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 217.77  on 181  degrees of freedom
## Residual deviance: 208.19  on 178  degrees of freedom
## AIC: 216.19
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(coef(fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        (Intercept) ENSG00000169347.15  ENSG00000179751.6  ENSG00000142615.7 
##          1.4838370          1.0803008          1.1877791          0.9045531&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thresh &amp;lt;- find_optimal_thresh(fit, data_genes, data_genes$probs, 
    TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Logistic%20with%203%20Genes-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get relevant metrics on fit
class_diag(data_genes$probs, data$Head_of_pancreas, 
    thresh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         acc      sens      spec       ppv        f1       auc
## 1 0.7417582 0.9923077 0.1153846 0.7371429 0.8459016 0.6445266&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate confusion matrix of fit
conf_matrix(data_genes$probs, data$Head_of_pancreas, 
    thresh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        truth
##           0   1
##   FALSE   6   1
##   TRUE   46 129&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit_density(fit, data_genes$Head_of_pancreas)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Logistic%20with%203%20Genes-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data_genes) + geom_roc(aes(d = as.numeric(Head_of_pancreas), 
    m = probs), n.cuts = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Logistic%20with%203%20Genes-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this one I picked three random genes with which to work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;log(odds of class 1) = 0.07724(ENSG00000169347.15) - 0.17209(ENSG00000175535.6) + 0.10031(ENSG00000164266.9) + 0.39463&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Log odds are hard to intrepret, so I exponentiate all of the coefficients.&lt;/p&gt;
&lt;p&gt;Controlling for ENSG00000175535.6 and ENSG00000164266.9, each one unit increase in ENSG00000169347.15 increases the odds of being class 1 by a factor of 1.0803008 (not significant)&lt;/p&gt;
&lt;p&gt;Controlling for ENSG00000169347.15 and ENSG00000164266.9, each one unit increase in ENSG00000175535.6 increases the odds of being class 1 by a factor of 1.1877791 (not significant)&lt;/p&gt;
&lt;p&gt;Controlling for ENSG00000169347.15 ENSG00000175535.6, each one unit increase in ENSG00000164266.9 increases the odds of being class 1 by a factor of 0.9045531 (not significant)&lt;/p&gt;
&lt;p&gt;Immediately, it’s clear that this model sacrifices specificity (0.1153846) for sensitivity (0.9923077). The model’s accuracy is 0.7417582 and its precision is 0.7371429 The F1-score is surprisingly high (0.8459016), which is never bad. The AUC is a lousy 0.6445266.&lt;/p&gt;
&lt;p&gt;The ROC curve has a very low AUC (0.6445266). This means that the model cannot achieve a high TPR without incurring a high FPR.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-predicting-head-of-pancreas-from-all-genes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6a. Predicting Head of Pancreas from All Genes&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pull out genes, head of pancreas
data_genes &amp;lt;- data %&amp;gt;% select(contains(&amp;quot;ENSG&amp;quot;), Head_of_pancreas)
# fit logistic model to all genes; predict log odds
# of Head_of_pancreas; maxit = 75 prevents model
# from failing with large numbers of variables
fit &amp;lt;- glm(Head_of_pancreas ~ ., family = &amp;quot;binomial&amp;quot;, 
    data = data_genes, control = list(maxit = 75))
# get class probabilities
data_genes$probs &amp;lt;- predict(fit, type = &amp;quot;response&amp;quot;)
summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Head_of_pancreas ~ ., family = &amp;quot;binomial&amp;quot;, data = data_genes, 
##     control = list(maxit = 75))
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.85033  -0.05722   0.11068   0.48014   1.84377  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept)        -1.563394   2.983896  -0.524  0.60032   
## ENSG00000164266.9   0.102611   0.277310   0.370  0.71136   
## ENSG00000169347.15  0.344227   0.418384   0.823  0.41065   
## ENSG00000175535.6  -0.667590   1.139099  -0.586  0.55783   
## ENSG00000215704.8   1.631599   1.326965   1.230  0.21886   
## ENSG00000211892.3  -0.027101   0.226791  -0.119  0.90488   
## ENSG00000243480.6  -1.252604   0.643884  -1.945  0.05173 . 
## ENSG00000096006.10 -0.307265   0.267720  -1.148  0.25109   
## ENSG00000118271.8   0.671077   0.707698   0.948  0.34300   
## ENSG00000219073.6  -0.824490   1.552025  -0.531  0.59526   
## ENSG00000066405.11 -0.555978   0.335348  -1.658  0.09733 . 
## ENSG00000122711.7  -0.026514   0.288477  -0.092  0.92677   
## ENSG00000108849.6   0.145380   0.196099   0.741  0.45848   
## ENSG00000170890.12 -2.180794   1.493387  -1.460  0.14421   
## ENSG00000256618.2   0.034362   0.158207   0.217  0.82805   
## ENSG00000175084.10  0.186929   0.189869   0.985  0.32486   
## ENSG00000254647.5   0.476140   0.325111   1.465  0.14304   
## ENSG00000134193.13  0.512320   0.248319   2.063  0.03910 * 
## ENSG00000275896.3  -0.096492   0.460265  -0.210  0.83395   
## ENSG00000167653.4   0.089175   0.156002   0.572  0.56757   
##  [ reached getOption(&amp;quot;max.print&amp;quot;) -- omitted 42 rows ]
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 217.77  on 181  degrees of freedom
## Residual deviance: 104.19  on 120  degrees of freedom
## AIC: 228.19
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thresh &amp;lt;- find_optimal_thresh(fit, data_genes, data_genes$probs, 
    TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Logistic%20All%20Genes-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get relevant metrics on fit
class_diag(data_genes$probs, data$Head_of_pancreas, 
    thresh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         acc      sens      spec       ppv        f1       auc
## 1 0.8901099 0.9538462 0.7307692 0.8985507 0.9253731 0.9322485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate confusion matrix of fit
conf_matrix(data_genes$probs, data$Head_of_pancreas, 
    thresh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        truth
##           0   1
##   FALSE  38   6
##   TRUE   14 124&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate ROC curve for fit
ggplot(data_genes) + geom_roc(aes(d = as.numeric(Head_of_pancreas), 
    m = probs), n.cuts = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Logistic%20All%20Genes-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit_density(fit, data_genes$Head_of_pancreas)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/Logistic%20All%20Genes-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are too many coefficients to count, but I’ll provide a template for their interpretation: &lt;strong&gt;variable&lt;/strong&gt; has a coefficient of &lt;strong&gt;coeff&lt;/strong&gt;. This means that for every one-unit increase in &lt;strong&gt;variable&lt;/strong&gt;, log odds of head of pancreas (increases/decreases) by &lt;strong&gt;coeff&lt;/strong&gt;. (if &lt;code&gt;Pr(&amp;gt;|z|)&lt;/code&gt; &amp;lt; 0.05: significant; else: not significant)&lt;/p&gt;
&lt;p&gt;The model classifies 38 of the 52 false values as false. It classifies 124 of the 130 positive values as positive.&lt;/p&gt;
&lt;p&gt;The model performs very well. It is somewhat more sensitive (0.95) than it is specific (0.73); it gets a high accuracy (0.89); it has a precision of 0.899; it has a great f1 (0.925). The AUC is a great 0.932.&lt;/p&gt;
&lt;p&gt;The optimal decision threshold for this model is at probability = 0.498; anything above is classified a Head of Pancreas sample. Anything below is non-head of pancreas.&lt;/p&gt;
&lt;p&gt;The ROC curve has a very large AUC (0.932). This means that the model can achieve a very high TPR while maintaining a relatively low FPR.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-10-fold-cv-with-all-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6b. 10 fold CV with all variables&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_genes &amp;lt;- data %&amp;gt;% select(contains(&amp;quot;ENSG&amp;quot;), Head_of_pancreas)
k = 10
temp &amp;lt;- data_genes[sample(nrow(data_genes)), ]
folds &amp;lt;- cut(seq(1:nrow(data_genes)), breaks = k, labels = F)
diags &amp;lt;- NULL
for (i in 1:k) {
    train &amp;lt;- temp[folds != i, ]  #create training set (all but fold i)
    test &amp;lt;- temp[folds == i, ]  #create test set (just fold i)
    truth &amp;lt;- test$Head_of_pancreas  #save truth labels from fold i
    fit &amp;lt;- glm(Head_of_pancreas ~ ., family = &amp;quot;binomial&amp;quot;, 
        data = train, control = list(maxit = 75))
    probs &amp;lt;- predict(fit, newdata = test, type = &amp;quot;response&amp;quot;)
    thresh &amp;lt;- find_optimal_thresh(fit, train, predict(fit, 
        type = &amp;quot;response&amp;quot;))
    diags &amp;lt;- rbind(diags, class_diag(probs, truth, 
        thresh))
}
summarize_all(diags, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         acc      sens      spec       ppv       f1       auc
## 1 0.6312865 0.7257792 0.3980952 0.7486476 0.731227 0.5923399&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 10-fold CV has exposed the degree to which the previous model had overfitted to the dataset. Even with optimal decision cutoffs, the AUC was only 0.632. Accuracy, sensitivity, specificity, precision, and f1 are 0.7186647, 0.442619, 0.7710556, and 0.7384776, respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-lasso&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6c. LASSO&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
data_genes &amp;lt;- data %&amp;gt;% select(contains(&amp;quot;ENSG&amp;quot;), Head_of_pancreas)
y &amp;lt;- as.matrix(data_genes$Head_of_pancreas)
preds &amp;lt;- model.matrix(Head_of_pancreas ~ ., data = data_genes)[, 
    -1] %&amp;gt;% scale
cv &amp;lt;- cv.glmnet(preds, y, family = &amp;quot;binomial&amp;quot;)
{
    plot(cv$glmnet.fit, &amp;quot;lambda&amp;quot;, label = TRUE)
    abline(v = log(cv$lambda.1se))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/projects/Project2/index_files/figure-html/LASSO-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lasso_fit &amp;lt;- glmnet(preds, y, family = &amp;quot;binomial&amp;quot;, 
    lambda = cv$lambda.1se)
# probs &amp;lt;- predict(lasso_fit, preds,
# type=&amp;#39;response&amp;#39;)
# class_diag(probs,data_genes$Head_of_pancreas,0.5)
# conf_matrix(probs,data_genes$Head_of_pancreas)
x &amp;lt;- coef(lasso_fit)
rownames(x)[x[, 1] &amp;gt; 0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(Intercept)&amp;quot;        &amp;quot;ENSG00000108849.6&amp;quot;  &amp;quot;ENSG00000175084.10&amp;quot;
## [4] &amp;quot;ENSG00000188257.9&amp;quot;  &amp;quot;ENSG00000172016.14&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LASSO selected four genes as effective predictors:“ENSG00000108849.6” “ENSG00000175084.10” “ENSG00000188257.9” “ENSG00000172016.14”. Using these variables to train a model will make it much more generalizable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d.-10-fold-cv-on-lasso-honed-logistic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6d. 10-fold CV on LASSO-honed Logistic Model&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_genes &amp;lt;- data %&amp;gt;% select(ENSG00000108849.6, ENSG00000175084.10, 
    ENSG00000188257.9, ENSG00000172016.14, Head_of_pancreas)
k = 10
temp &amp;lt;- data_genes[sample(nrow(data_genes)), ]
folds &amp;lt;- cut(seq(1:nrow(data_genes)), breaks = k, labels = F)
diags &amp;lt;- NULL
for (i in 1:k) {
    train &amp;lt;- temp[folds != i, ]  #create training set (all but fold i)
    test &amp;lt;- temp[folds == i, ]  #create test set (just fold i)
    truth &amp;lt;- test$Head_of_pancreas  #save truth labels from fold i
    fit &amp;lt;- glm(Head_of_pancreas ~ ., family = &amp;quot;binomial&amp;quot;, 
        data = train)
    probs &amp;lt;- predict(fit, newdata = test, type = &amp;quot;response&amp;quot;)
    thresh &amp;lt;- find_optimal_thresh(fit, train, predict(fit, 
        type = &amp;quot;response&amp;quot;))
    diags &amp;lt;- rbind(diags, class_diag(probs, truth, 
        thresh))
}
summarize_all(diags, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         acc      sens      spec      ppv        f1       auc
## 1 0.7312865 0.9149567 0.2194444 0.745767 0.8189543 0.6573088&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I suppose this is the best I can do given the tools we’ve learned in this course. The AUC is terrible (0.680). While sensitivity is great (0.93), specificity is non-existent (0.1). This tells me that the decision threshold chosen for my model is basically near 0, and it is classifying everything as Head of pancreas. Accuracy is bad, but not atrocious (0.69). Precision and f1 score are 0.72 and 0.81, respectively.&lt;/p&gt;
&lt;p&gt;Although this model gets lower scores than some of its predecessors, it is much more generalizable than those previous models and will thus perform better than them on new data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.0 (2019-04-26)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.15.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] rstatix_0.5.0        interactions_1.1.3   factoextra_1.0.7    
##  [4] circlize_0.4.9       ComplexHeatmap_2.0.0 reshape2_1.4.4      
##  [7] prettyR_2.2-3        glmnet_4.0-2         Matrix_1.2-18       
## [10] plotROC_2.2.1        lmtest_0.9-38        zoo_1.8-8           
## [13] sandwich_3.0-0       forcats_0.5.0        stringr_1.4.0       
## [16] dplyr_1.0.0          purrr_0.3.4          tidyr_1.1.0         
## [19] tibble_3.0.1         ggplot2_3.3.1        tidyverse_1.3.0     
## [22] readr_1.3.1         
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-148        fs_1.4.1            lubridate_1.7.9    
##  [4] RColorBrewer_1.1-2  httr_1.4.1          tools_3.6.0        
##  [7] backports_1.1.7     utf8_1.1.4          R6_2.4.1           
## [10] mgcv_1.8-31         DBI_1.1.0           colorspace_1.4-1   
## [13] GetoptLong_0.1.8    withr_2.2.0         tidyselect_1.1.0   
## [16] curl_4.3            compiler_3.6.0      cli_2.0.2          
## [19] rvest_0.3.5         formatR_1.7         xml2_1.3.2         
## [22] labeling_0.3        bookdown_0.19       scales_1.1.1       
## [25] digest_0.6.25       foreign_0.8-72      rmarkdown_2.2      
## [28] rio_0.5.16          pkgconfig_2.0.3     htmltools_0.4.0    
## [31] dbplyr_1.4.4        rlang_0.4.6         GlobalOptions_0.1.1
## [34] readxl_1.3.1        rstudioapi_0.11     farver_2.0.3       
## [37] shape_1.4.4         generics_0.0.2      jsonlite_1.6.1     
## [40] zip_2.0.4           car_3.0-8           magrittr_1.5       
## [43] Rcpp_1.0.5          munsell_0.5.0       fansi_0.4.1        
## [46] abind_1.4-5         lifecycle_0.2.0     stringi_1.4.6      
## [49] yaml_2.2.1          carData_3.0-4       plyr_1.8.6         
## [52] blob_1.2.1          parallel_3.6.0      ggrepel_0.8.2      
## [55] crayon_1.3.4        lattice_0.20-41     haven_2.3.1        
## [58] splines_3.6.0       jtools_2.1.0        pander_0.6.3       
## [61] hms_0.5.3           knitr_1.28          pillar_1.4.4       
## [64] rjson_0.2.20        codetools_0.2-16    reprex_0.3.0       
## [67] glue_1.4.1          evaluate_0.14       blogdown_0.19      
## [70] data.table_1.12.8   modelr_0.1.8        png_0.1-7          
## [73] vctrs_0.3.1         foreach_1.5.0       cellranger_1.1.0   
## [76] gtable_0.3.0        clue_0.3-57         assertthat_0.2.1   
## [79] openxlsx_4.1.5      xfun_0.14           broom_0.5.6        
## [82] survival_3.1-12     iterators_1.0.12    cluster_2.1.0      
## [85] ellipsis_0.3.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2020-12-08 23:18:44 CST&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                                            sysname 
##                                                                                           &amp;quot;Darwin&amp;quot; 
##                                                                                            release 
##                                                                                           &amp;quot;19.6.0&amp;quot; 
##                                                                                            version 
## &amp;quot;Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64&amp;quot; 
##                                                                                           nodename 
##                                                                                  &amp;quot;Extension.local&amp;quot; 
##                                                                                            machine 
##                                                                                           &amp;quot;x86_64&amp;quot; 
##                                                                                              login 
##                                                                                             &amp;quot;root&amp;quot; 
##                                                                                               user 
##                                                                                       &amp;quot;ryanbailey&amp;quot; 
##                                                                                     effective_user 
##                                                                                       &amp;quot;ryanbailey&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploratory Data Analysis in R</title>
      <link>/projects/project1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/projects/project1/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;eid-reb3566&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;EID: REB3566&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;0. Introduction&lt;/h1&gt;
&lt;p&gt;Three datasets are combined and explored in this R markdown file. The &lt;a href=&#34;https://data.world/adamhelsinger/cancer-rates-by-u-s-state&#34;&gt;first dataset&lt;/a&gt; contains cancer mortality and incidence by states in the United States. The &lt;a href=&#34;https://data.world/wendyhe/how-religious-is-your-state&#34;&gt;second dataset&lt;/a&gt; has relevant religiosity rankings (number of congregations, number of adherents per 1,000 people) by state. The final dataset lists each state by region (West, Southeast, Southwest, Northeast, and Midwest). The first two datasets were downloaded from data.world. The Regions dataset was constructed by hand from information found on &lt;a href=&#34;https://www.ducksters.com/geography/us_states/us_geographical_regions.php&#34;&gt;ducksters.com/geography&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I would like to investigate relationships between state/regional religiosity and cancer rates and incidences. I do not expect to find any prominent associations within the data. The datasets neglect dozens (if not hundreds) of potential confounds and biases. If cancer incidence or mortality rates are correlated with religiosity, this is likely because Bible Belt states have more inadequate healthcare than other US regions. Mortality and incidence may differ to some extent by region, but this difference is unlikely to be statistically significant.&lt;/p&gt;
&lt;div id=&#34;import-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import libraries&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specific data import instructions
library(readr)

# data wrangling
library(tidyverse)

# plotting
library(ggplot2)
library(plotly)
library(GGally)

# for making clustered correlation heatmap
library(prettyR)
library(reshape2)
library(ComplexHeatmap)
library(circlize)


# for Kmeans and PAM Clustering
library(cluster)

# for PCA Biplot
library(factoextra)

# for Pretty Workflows
library(DiagrammeR)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Workflow&lt;/h2&gt;
&lt;p&gt;I realize that there’s a lot going on with my Project. It’s very figure-heavy, which I know makes it feel more cluttered; I just hope makes it also makes it more informative. I included my workflow to try and clarify my process and to let you know what things to expect and the order to expect them in. It was also a really cool opportunity to use DiagrameR.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grViz(&amp;quot;digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = &amp;#39;@@1&amp;#39;]
      tab2 [label = &amp;#39;@@2&amp;#39;]
      tab3 [label = &amp;#39;@@3&amp;#39;]
      tab4 [label = &amp;#39;@@4&amp;#39;]
      tab5 [label = &amp;#39;@@5&amp;#39;]
      tab6 [label = &amp;#39;@@6&amp;#39;]
      tab7 [label = &amp;#39;@@7&amp;#39;]
      tab8 [label = &amp;#39;@@8&amp;#39;]
      tab9 [label = &amp;#39;@@9&amp;#39;]
      tab10 [label = &amp;#39;@@10&amp;#39;]
      tab11 [label = &amp;#39;@@11&amp;#39;]
      tab12 [label = &amp;#39;@@12&amp;#39;]
      tab13 [label = &amp;#39;@@13&amp;#39;]
      tab14 [label = &amp;#39;@@14&amp;#39;]
      tab15 [label = &amp;#39;@@15&amp;#39;]

      # edge definitions with the node IDs
      tab1 -&amp;gt; tab2 -&amp;gt; tab3 -&amp;gt; tab4 -&amp;gt; tab5 -&amp;gt; tab6 -&amp;gt; tab7 -&amp;gt; tab8;
      tab8 -&amp;gt; tab9;
      tab8 -&amp;gt; tab10 -&amp;gt; tab12 -&amp;gt; tab13;
      tab8 -&amp;gt; tab11 -&amp;gt; tab14 -&amp;gt; tab15;
      tab9 -&amp;gt; tab13;
      tab9 -&amp;gt; tab15
      }

      [1]: &amp;#39;Import libraries&amp;#39;
      [2]: &amp;#39;Import All 3 Datasets&amp;#39;
      [3]: &amp;#39;Join All 3 Datasets&amp;#39;
      [4]: &amp;#39;Drop Irrelevant Variables from Joined Dataset&amp;#39;
      [5]: &amp;#39;Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot&amp;#39;
      [6]: &amp;#39;Visualize Different Variables and the Relationships Between Them&amp;#39;
      [7]: &amp;#39;Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot&amp;#39;
      [8]: &amp;#39;Extract Numeric Variables From Dataset&amp;#39;
      [9]: &amp;#39;Run PCA Only on Numeric Variables&amp;#39;
      [10]: &amp;#39;Validate Optimal K for Kmeans&amp;#39;
      [11]: &amp;#39;Validate Optimal K for PAM&amp;#39;
      [12]: &amp;#39;Perform Kmeans Clustering Using Optimal K&amp;#39;
      [13]: &amp;#39;Visualize Kmeans Clusters In Feature Space&amp;#39;
      [14]: &amp;#39;Perform PAM Clustering Using Optimal K&amp;#39;
      [15]: &amp;#39;Visualize PAM Clusters In Feature Space&amp;#39;
      &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph flowchart {\n      # node definitions with substituted label text\n      node [fontname = Helvetica, shape = rectangle]        \n      tab1 [label = \&#34;Import libraries\&#34;]\n      tab2 [label = \&#34;Import All 3 Datasets\&#34;]\n      tab3 [label = \&#34;Join All 3 Datasets\&#34;]\n      tab4 [label = \&#34;Drop Irrelevant Variables from Joined Dataset\&#34;]\n      tab5 [label = \&#34;Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot\&#34;]\n      tab6 [label = \&#34;Visualize Different Variables and the Relationships Between Them\&#34;]\n      tab7 [label = \&#34;Summarize Dataset with Summary Stats for Each Variable and GGPairs Plot\&#34;]\n      tab8 [label = \&#34;Extract Numeric Variables From Dataset\&#34;]\n      tab9 [label = \&#34;Run PCA Only on Numeric Variables\&#34;]\n      tab10 [label = \&#34;Validate Optimal K for Kmeans\&#34;]\n      tab11 [label = \&#34;Validate Optimal K for PAM\&#34;]\n      tab12 [label = \&#34;Perform Kmeans Clustering Using Optimal K\&#34;]\n      tab13 [label = \&#34;Visualize Kmeans Clusters In Feature Space\&#34;]\n      tab14 [label = \&#34;Perform PAM Clustering Using Optimal K\&#34;]\n      tab15 [label = \&#34;Visualize PAM Clusters In Feature Space\&#34;]\n\n      # edge definitions with the node IDs\n      tab1 -&gt; tab2 -&gt; tab3 -&gt; tab4 -&gt; tab5 -&gt; tab6 -&gt; tab7 -&gt; tab8;\n      tab8 -&gt; tab9;\n      tab8 -&gt; tab10 -&gt; tab12 -&gt; tab13;\n      tab8 -&gt; tab11 -&gt; tab14 -&gt; tab15;\n      tab9 -&gt; tab13;\n      tab9 -&gt; tab15\n      }&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;import-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import datasets&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read datasets
cancer_states &amp;lt;- read_csv(&amp;quot;datasets/cancer_states.csv&amp;quot;)
religion_states &amp;lt;- read_csv(&amp;quot;datasets/religion_states.csv&amp;quot;)
region_states &amp;lt;- read_csv(&amp;quot;datasets/region_states.csv&amp;quot;)

# dataset dimensions
cancer_states %&amp;gt;% dim&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 50  5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;religion_states %&amp;gt;% dim&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 50  8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region_states %&amp;gt;% dim&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 50  2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;joiningmerging&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Joining/Merging&lt;/h1&gt;
&lt;p&gt;Since I have three datasets, I have to perform two joins. I used inner_join() for both because I only want complete cases. NAs were not a problem as the datasets were relatively small, had no missing values, and used the same identifiers for states.&lt;/p&gt;
&lt;div id=&#34;a.-joining-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2a. Joining Datasets&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# none of the datasets have a colname for the first column; join each dataset by this column
data &amp;lt;- inner_join(cancer_states, religion_states, by = c(&amp;quot;X1&amp;quot; = &amp;quot;X1&amp;quot;)) %&amp;gt;% inner_join(region_states, by = c(&amp;quot;X1&amp;quot; = &amp;quot;X1&amp;quot;))
data %&amp;gt;% glimpse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 50
## Columns: 13
## $ X1                                      &amp;lt;chr&amp;gt; &amp;quot;Alaska&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;Arkansa…
## $ mo_Rate                                 &amp;lt;dbl&amp;gt; 173.1, 182.1, 189.6, 146.4, 1…
## $ mo_range_low                            &amp;lt;dbl&amp;gt; 164.2, 174.5, 174.5, 127.9, 1…
## $ mo_range_high                           &amp;lt;dbl&amp;gt; 174.4, 199.3, 199.3, 155.3, 1…
## $ in_Rate                                 &amp;lt;dbl&amp;gt; 406.6, 437.9, 456.2, 379.8, 3…
## $ in_range_low                            &amp;lt;dbl&amp;gt; 369.9, 420.4, 447.0, 369.9, 3…
## $ in_range_high                           &amp;lt;dbl&amp;gt; 416.5, 445.7, 461.0, 416.5, 4…
## $ total_number_of_congregations           &amp;lt;dbl&amp;gt; 1246, 10514, 6697, 4673, 2355…
## $ total_number_of_adherents               &amp;lt;dbl&amp;gt; 240833, 3007553, 1614357, 237…
## $ rates_of_adherence_per_1_000_population &amp;lt;dbl&amp;gt; 339.09, 629.23, 553.64, 372.3…
## $ of_adherents                            &amp;lt;dbl&amp;gt; 0.33909, 0.62923, 0.55364, 0.…
## $ of_adults_who_are_highly_religious      &amp;lt;dbl&amp;gt; 0.45, 0.77, 0.70, 0.53, 0.49,…
## $ Region                                  &amp;lt;chr&amp;gt; &amp;quot;West&amp;quot;, &amp;quot;Southeast&amp;quot;, &amp;quot;Southea…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;and-3-tidying-and-wrangling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1 and 3: Tidying and Wrangling&lt;/h1&gt;
&lt;p&gt;I perform data tidying after joining. Here I use pivot functions to make summary statistics more digestible. I use pivot_longer to reformat the summarized output from a single row with ~54 variables (one for each variable and each summary stat) to one with 3 variables (variable, stat, and value) and 54 rows. I pivot wider to give a final table of 8 variables (each numeric variable in the dataset) and 9 rows (each type of summary statistic used).&lt;/p&gt;
&lt;p&gt;I also drop mo/in_range_high/low and total number of religious adherence. These first four variables correspond to the mortality and incidence 95% confidence intervals. The dataset contains a rate of adherence variable that makes total adherence obsolete (due to its bias towards more populous states).&lt;/p&gt;
&lt;div id=&#34;a.-summary-statistics---6-core-dplyr-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3a. Summary Statistics - 6 Core dplyr Functions&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# xx_range_high/low are just the 95% CI for the mortality and incidence rates
# total number of adherents is highly dependent on state population; this is dropped because there is a rates of adherence per 1000 people variable
data &amp;lt;- data %&amp;gt;% select(!c(mo_range_low,mo_range_high,in_range_low,in_range_high, total_number_of_adherents))

# use of all 6 core dplyr functions
data %&amp;gt;% group_by(Region) %&amp;gt;% summarize(total_cong = sum(total_number_of_congregations), mean_cong = mean(total_number_of_congregations), sd_cong = sd(total_number_of_congregations)) %&amp;gt;% filter(Region != &amp;quot;West&amp;quot;) %&amp;gt;% select(!c(total_cong)) %&amp;gt;% arrange(mean_cong) %&amp;gt;% mutate(plus_one_sd = mean_cong + sd_cong,minus_one_sd = mean_cong - sd_cong )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 5
##   Region    mean_cong sd_cong plus_one_sd minus_one_sd
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Northeast     4789.   5276.      10065.        -487.
## 2 Midwest       6812.   3957.      10769.        2854.
## 3 Southeast     9608    3683.      13291.        5925.
## 4 Southwest    10506.  11713.      22220.       -1207.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-and-3b-summary-statistics---summarize&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1a and 3b: Summary Statistics - Summarize&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is an infinitely wordier way to create the summary tables I love in Python Pandas; every variable gets a full suite of summary statistics
N &amp;lt;- nrow(data)

data %&amp;gt;% summarize_if(is.numeric,list(xxxmean = mean, xxxsd = sd ,  xxxse = function(x) sd(x) / sqrt(N), xxxmin = min,xxxper25 = function(x) quantile(x,0.25), xxxmedian = median,xxxper75 = function(x) quantile(x, 0.75),xxxmax =  max, xxxrange = function(x) max(x) - min(x))) %&amp;gt;% pivot_longer(cols = everything()) %&amp;gt;% separate(name, into = c(&amp;quot;variable&amp;quot;, &amp;quot;stat&amp;quot;), sep = &amp;quot;_xxx&amp;quot;) %&amp;gt;% pivot_wider(names_from = variable, values_from = value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 7
##   stat  mo_Rate in_Rate total_number_of… rates_of_adhere… of_adherents
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 mean   165.    440.              6886.            483.        0.483 
## 2 sd      15.6    31.1             5840.            103.        0.103 
## 3 se       2.20    4.39             826.             14.6       0.0146
## 4 min    128.    370.               677             276.        0.276 
## 5 per25  155.    417.              2432             400.        0.400 
## 6 medi…  164.    446.              5699             507.        0.507 
## 7 per75  174.    461.              9406             553.        0.553 
## 8 max    199.    514.             27848             791.        0.791 
## 9 range   71.4   144.             27171             515.        0.515 
## # … with 1 more variable: of_adults_who_are_highly_religious &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% group_by(Region) %&amp;gt;% summarize_if(is.numeric,list(xxxmean = mean, xxxsd = sd ,  xxxse = function(x) sd(x) / sqrt(N), xxxmin = min,xxxper25 = function(x) quantile(x,0.25), xxxmedian = median,xxxper75 = function(x) quantile(x, 0.75),xxxmax =  max, xxxrange = function(x) max(x) - min(x))) %&amp;gt;% pivot_longer(cols = !Region) %&amp;gt;% separate(name, into = c(&amp;quot;variable&amp;quot;, &amp;quot;stat&amp;quot;), sep = &amp;quot;_xxx&amp;quot;) %&amp;gt;% pivot_wider(names_from = variable, values_from = value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 45 x 8
##    Region stat  mo_Rate in_Rate total_number_of… rates_of_adhere… of_adherents
##    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Midwe… mean   166.    450.              6812.           526.        0.526  
##  2 Midwe… sd       9.85   11.6             3957.            70.5       0.0705 
##  3 Midwe… se       1.39    1.65             560.             9.97      0.00997
##  4 Midwe… min    151.    431.              1498            421.        0.421  
##  5 Midwe… per25  159.    442.              4302.           480.        0.480  
##  6 Midwe… medi…  166.    450.              6016.           537.        0.537  
##  7 Midwe… per75  173.    458.              9176            558.        0.558  
##  8 Midwe… max    179.    472.             13606            671.        0.671  
##  9 Midwe… range   28.6    40.5            12108            250.        0.250  
## 10 North… mean   163.    466.              4789.           457.        0.457  
## # … with 35 more rows, and 1 more variable:
## #   of_adults_who_are_highly_religious &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Visualizing&lt;/h1&gt;
&lt;div id=&#34;ggpairs-a-hierarchically-clustered-correlation-heatmap-faceted-boxplot-violin-plot-and-faceted-scatterplot-bubbleplot&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;GGPairs, A Hierarchically Clustered Correlation Heatmap, Faceted Boxplot, Violin Plot, and Faceted Scatterplot (Bubbleplot)&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-ggpairs-plot-to-view-relationships-between-each-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4a. GGPairs Plot to View Relationships Between Each Variables&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pairwise summary plots like ggpairs are great for getting a mile-high view of the dataset. They fall apart when you get more than 8 or 9 variables.
data %&amp;gt;% select(!X1) %&amp;gt;% ggpairs(aes(color = as.factor(data$Region)), progress = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;1440&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Few significant relationships jump out in the GGPairs plot. There appears to be a significant, strong, and positive correlation between cancer mortality rate and the total number of congregations in the midwest. Also, there seems to be a significant, strong, and negative correlation between cancer mortality rate and adherence rate per 1000 people in the midwest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-hierarchically-clustered-correlation-heatmap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4b Hierarchically Clustered Correlation Heatmap&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I personally find correlation geom_tiles hard to read. I found this package called ComplexHeatmap which will Hierarchically cluster the correlation values. This places similar variable combinations together and makes correlations quite a bit easier to assess.

# define color function
col_fun = colorRamp2(c(-1, 0, 1), c(&amp;quot;green&amp;quot;, &amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;))

# generate a clustered heatmap for the correlation matrix
Heatmap(data %&amp;gt;% select_if(is.numeric) %&amp;gt;% cor(use=&amp;quot;pair&amp;quot;) %&amp;gt;% as.matrix, 
        name = &amp;quot;Correlation Matrix of Numeric Variables&amp;quot;, #title of legend
        column_title = &amp;quot;&amp;quot;, row_title = &amp;quot;&amp;quot;,
        row_names_gp = gpar(fontsize = 6), 
        column_names_gp = gpar(fontsize = 6),
        col = col_fun) # Text size for row names&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As identified in the ggpairs plot, there are few strong correlations between any variables (save for the variables that are obviously related e.g., of_adherents and rates of adherence per 1000). This will show the same thing as a geom_tile, but here, it’s a little easier to see the relationships between variables since similarly correlated variables are “clustered” closely together.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Interestingly (or maybe not interestingly) the variable correlations cluster based on the dataset from which they originate. This is likely because mortality and incidence often go hand in hand.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-boxplot-using-statsummary-and-facet_wrap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4c. Boxplot Using stat=“summary” and facet_wrap&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- function(x) {
  r &amp;lt;- quantile(x, probs = c(0.10, 0.25, 0.5, 0.75, 0.90))
  names(r) &amp;lt;- c(&amp;quot;ymin&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;middle&amp;quot;, &amp;quot;upper&amp;quot;, &amp;quot;ymax&amp;quot;)
  r
}
order &amp;lt;- c(&amp;quot;low&amp;quot;,&amp;quot;moderate&amp;quot;,&amp;quot;high&amp;quot;)
df &amp;lt;- data %&amp;gt;% mutate(mo_rate_f= as.factor(cut(mo_Rate, breaks=c(115,150,170, 250), labels = order,levels=order))) %&amp;gt;% group_by(Region) %&amp;gt;% as.data.frame

df %&amp;gt;% ggplot(aes(Region,total_number_of_congregations)) + geom_boxplot(aes(fill = Region), stat = &amp;quot;summary&amp;quot;, fun.data = f, position = position_dodge(1), alpha = 1) + scale_fill_viridis_d() + theme_minimal() + ylab(&amp;quot;Total Number of Congregations&amp;quot;)+ facet_wrap(~mo_rate_f) + coord_flip() + ggtitle(&amp;quot;Total Number of Congregations By Region&amp;quot;, subtitle = &amp;quot;Faceted by low, moderate, and high mortality rates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here are horizontal boxplots comparing the number of congregations by region and low, moderate, high cancer mortality rates. The low number of samples causes some boxplots to spawn as just their median. The only stand-out difference between groups is between the West and Southeast regions in the moderate mortality rate group.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d.-violin-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4d. Violin Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% ggplot(aes(x=Region, y=in_Rate)) +
  geom_violin(aes(x=Region, y=in_Rate, fill = Region) ,alpha = 0.5,trim = F, draw_quantiles=0:2/2) + geom_jitter(aes(color = rates_of_adherence_per_1_000_population),alpha = 1, size = 3) + scale_color_gradient(low = &amp;#39;blue&amp;#39;, high = &amp;#39;red&amp;#39;)+ theme_minimal() + ylab(&amp;quot;Cancer Incidence Rate&amp;quot;) + xlab(&amp;#39;Region&amp;#39;) + ggtitle(&amp;quot;Cancer Incidence by Region&amp;quot;) + scale_fill_viridis_d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot compares cancer incidence and rates of adherence between regions. We can see that there is a much higher cancer incidence in the Northeast than there is in the West. There is also greater rates of adherence in the Southeast than there is in the West.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e.-faceted-scatterplot-with-linear-models-and-custom-axis-ticks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4e. Faceted Scatterplot with linear models and custom axis ticks&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% ggplot(aes(x=mo_Rate, y=rates_of_adherence_per_1_000_population)) + geom_point(aes(size = mo_Rate, color = Region),alpha = 0.5) + scale_size(range = c(.1, 7), name=&amp;quot;Cancer Incidence&amp;quot;) + geom_smooth(aes(color= Region,group=Region), method = &amp;quot;lm&amp;quot;, se=T,alpha = 0.2)+ scale_y_continuous(breaks = seq(0,700, 200), limits = c(0,700)) + scale_color_viridis_d() + theme_minimal() + facet_wrap(~Region) + xlab(&amp;quot;Mortality Rate&amp;quot;) + ylab(&amp;quot;Rates of Adherence (per 1000 people)&amp;quot;) + ggtitle(&amp;quot;Rates of Religous Adherence by Mortality Rate&amp;quot;, subtitle = &amp;quot;Faceted by Region, Sized by Cancer Incidence, Linear Model Fitted by Region&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Facet scatterplot of cancer mortality rate and rate of religious adherence. They were grouped by Region, sized by cancer incidence rate. LM fitted to each group. This plot clarifies slight relationships between religiosity and cancer mortality rates. As expected, these relationships tend to be weak.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dimensionality-reduction-and-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Dimensionality Reduction and Clustering&lt;/h1&gt;
&lt;div id=&#34;a.-running-pca-on-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5a. Running PCA on Dataset&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(421)
data_pca &amp;lt;-data %&amp;gt;% select_if(is.numeric) %&amp;gt;% scale %&amp;gt;% princomp 
data_pca_df&amp;lt;- data.frame(PC1=data_pca$scores[, 1], PC2=data_pca$scores[, 2], PC3 = data_pca$scores[, 3]) %&amp;gt;%mutate(name = as.character(data$X1), Region = as.character(data$Region))

eigval &amp;lt;-  data_pca$sdev^2 
varprop=round(eigval/sum(eigval), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-assess-variance-encompassed-by-each-principal-component&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5a. Assess Variance Encompassed by Each Principal Component&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() + geom_bar(aes(y=varprop, x=1:6), stat=&amp;quot;identity&amp;quot;) + xlab(&amp;quot;&amp;quot;) + geom_path(aes(y=varprop, x=1:6)) + geom_text(aes(x=1:6, y=varprop, label=round(varprop, 2)), vjust=1, col=&amp;quot;white&amp;quot;, size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10, limit = c(0.5,5.5)) + ggtitle(&amp;quot;Variation Within Each Prinicpal Component&amp;quot;) + ylab(&amp;quot;Variation&amp;quot;) + xlab(&amp;quot;Principal Components&amp;quot;) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that the first two principal components encompass ~68% of my data’s variation. I can capture up to ~ 86% variation if I visualize my data with the first three principal components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a.-visualizing-data-by-region-after-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5a. Visualizing Data By Region After PCA&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_pca_biplot(data_pca, habillage = data_pca_df$Region,label =&amp;quot;var&amp;quot;) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_pca_df %&amp;gt;% plot_ly(x=~PC1, y=~PC2, z=~PC3, type=&amp;quot;scatter3d&amp;quot;, mode=&amp;quot;markers&amp;quot;, color = ~Region) %&amp;gt;% layout(scene = list(xaxis = list(title = &amp;#39;PC1 (0.44)&amp;#39;), yaxis = list(title = &amp;#39;PC2 (0.24)&amp;#39;), zaxis = list(title = &amp;#39;PC3 (0.18)&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;68e85488b305&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;68e85488b305&#34;,&#34;attrs&#34;:{&#34;68e85488b305&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;z&#34;:{},&#34;mode&#34;:&#34;markers&#34;,&#34;color&#34;:{},&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;scatter3d&#34;}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:&#34;PC1 (0.44)&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;PC2 (0.24)&#34;},&#34;zaxis&#34;:{&#34;title&#34;:&#34;PC3 (0.18)&#34;}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:true},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;x&#34;:[0.822306348064632,1.07190906297059,-0.147141037458331,0.199294321369512,-0.5486819703151,0.514633853192868,0.713532673229052,1.37152976013352,0.52640377274337,0.267361548571376,0.944054171291118,0.231128778424715],&#34;y&#34;:[0.445263093303644,0.187234431605288,0.755372173672134,-0.023831098779958,0.493399345359134,-0.576278812636121,0.69547339300874,-1.80817244966095,-0.493796508091662,0.952726130821671,-0.793837447599091,0.00443657492663399],&#34;z&#34;:[-0.880936600577315,-0.0176822707947886,0.660594401635903,-0.501516275747051,0.687434704339677,-1.0688829549961,0.455617103016485,-1.75352538943136,-0.99390360205101,1.14931647247675,-1.24707169997073,-0.97388216592004],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;Midwest&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[-0.47822233006605,-0.664721550664497,-0.110640822935078,-0.818694302541133,-2.87978600311518,-2.51086753433792,0.744821107412191,0.334619938495907,1.22504178768941,0.552040137818934,-2.66923993620963],&#34;y&#34;:[-0.412077651900541,1.44721664016657,-0.549686882245433,0.295456128203461,2.256782135688,0.915242873299899,-0.134875414259115,-0.0326198407507055,0.485101333969798,0.637158564961516,0.932670344671229],&#34;z&#34;:[-1.61515979102661,-1.07851228349137,-1.81410104491278,0.128354444299685,-0.625537096908529,-1.15961820470845,-0.933292390651044,-0.144887248941461,0.252609340024118,-1.60339094145845,-0.787533889935342],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;Northeast&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[2.94216287467319,1.94743472870561,-1.03793615459785,1.13682464550325,1.88563997508177,2.66113290298741,2.77536371638002,0.844917543663161,1.2269703929894,2.1436573366832,-0.184957946854164,-0.229940207158274],&#34;y&#34;:[-0.0752557943122514,1.05577150697445,-0.598168804619145,0.22413553987748,2.73995454203208,1.15384861898201,1.30703210104687,0.261123615247009,0.102537708857827,0.613716038538981,-0.373052886393018,2.35693596974787],&#34;z&#34;:[0.666655165781602,0.191884751196139,1.8314226947022,0.798259695892016,-0.375985124631002,-0.458193452198609,0.263255998415282,1.41760066037626,0.670218397936242,0.979075178367292,1.09151752134985,0.89039417688404],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;Southeast&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[-2.11131140307473,-0.77240247364298,2.02770365941909,1.62545414928106],&#34;y&#34;:[-1.29076477750668,-2.22836665316224,0.350305139464718,-1.72861031213258],&#34;z&#34;:[0.866597831676071,0.162891600506687,0.019459868729898,2.9202965961821],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;Southwest&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[-2.29319772221018,-0.528766334892917,-2.33468913286766,-2.08356305271039,-0.350059644500599,-1.84031336754883,-2.44464391177211,-2.48042648733445,2.62393333417113,-2.03618441140133,-1.80348478273613],&#34;y&#34;:[0.592759393789722,-1.62733269142585,-1.36167153236652,-1.52225493577275,-0.871574691313242,-0.0217521523986969,-0.375858926629271,0.346329375583748,-4.18092893143258,0.414596230007585,-0.941809748419655],&#34;z&#34;:[0.256445374239515,2.35927179463706,0.280545018368283,-0.433248944726638,-0.655316679849994,-0.304095670007428,0.722945673710484,0.713693792291879,-1.274701741879,0.1869624137359,0.077654794043675],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;West&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(166,216,84,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(166,216,84,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(166,216,84,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(166,216,84,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(166,216,84,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(166,216,84,1)&#34;},&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;This is the first of three plot pairs. I am showing a biplot and 3D scatterplot of the scaled data (colored by Region).&lt;/p&gt;
&lt;p&gt;From the biplot, we can ascertain that PC1 is positively related to every numeric variable in the dataset. PC2 is strongly positively associated with mortality and incidence rates but is somewhat negatively related to rates_of_adherence and total number of congregations.&lt;/p&gt;
&lt;p&gt;I included the 3D scatterplot to help understand the data. While two principal components are sufficient to glean ~70% of the data’s variation, the addition of a third principal component yields about ~87% of the variation.&lt;/p&gt;
&lt;p&gt;As for the distribution of the points by Region, some regions are very well defined in the feature space. The West and Southeast, for instance, are well separated from the other points; Midwest, Northeast, and Southwest are less well-defined.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-validate-optimal-k-for-kmeans&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5b. Validate Optimal K for Kmeans&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max_k = 11
wss&amp;lt;-vector() 
sil_width&amp;lt;-vector() 
for(i in 2:max_k){
temp&amp;lt;- data %&amp;gt;% select_if(is.numeric) %&amp;gt;% scale %&amp;gt;% kmeans(i)
wss[i]&amp;lt;-temp$tot.withinss
sil &amp;lt;- silhouette(temp$cluster,dist(data))
sil_width[i]&amp;lt;-mean(sil[,3]) 
}

ggplot()+geom_point(aes(x=1:max_k,y=wss))+geom_path(aes(x=1:max_k,y=wss))+
  xlab(&amp;quot;clusters&amp;quot;)+scale_x_continuous(breaks=1:max_k) + xlim(2,max_k) + xlim(2,max_k) + xlab(&amp;quot;K&amp;quot;) + ylab(&amp;quot;WSS&amp;quot;) + theme_minimal() + ggtitle(&amp;quot;WSS Silhouette Width by K&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale for &amp;#39;x&amp;#39; is already present. Adding another scale for &amp;#39;x&amp;#39;, which will
## replace the existing scale.
## Scale for &amp;#39;x&amp;#39; is already present. Adding another scale for &amp;#39;x&amp;#39;, which will
## replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot()+geom_line(aes(x=1:max_k,y=sil_width))+scale_x_continuous(name=&amp;quot;k&amp;quot;,breaks=1:max_k)+ xlim(2,max_k) + xlab(&amp;quot;K&amp;quot;) + ylab(&amp;quot;Mean Silhouette Width&amp;quot;) + theme_minimal() + ggtitle(&amp;quot;Mean Silhouette Width by K&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale for &amp;#39;x&amp;#39; is already present. Adding another scale for &amp;#39;x&amp;#39;, which will
## replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on the average silhouette width plot and the WSS plot, the optimal value of K is 2. This value of K corresponds to an average silhouette width of ~0.11 and a WSS of ~200.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-assess-cluster-goodness-with-mesa-themed-silhouette-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5b. Assess Cluster “Goodness” with (Mesa-themed) Silhouette Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km &amp;lt;- data %&amp;gt;% select_if(is.numeric) %&amp;gt;%scale %&amp;gt;% kmeans(2)
ss &amp;lt;- silhouette(km$cluster, dist(data%&amp;gt;% select_if(is.numeric)))
plot(ss,col = c(&amp;#39;#E53D57&amp;#39;, &amp;quot;#E68E0C&amp;quot;), border = NA, main = &amp;quot;Silhouette Plot of Kmeans Clusters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the clusters look absolutely wretched. Average silhouette width is 0.11.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-visualizing-kmeans-clusters-in-feature-space-with-principal-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5b. Visualizing Kmeans Clusters In Feature Space (with Principal Components)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_pca_df_km &amp;lt;- data_pca_df %&amp;gt;% mutate(cluster=as.factor(km$cluster))
fviz_pca_biplot(data_pca, habillage = data_pca_df_km$cluster,label =&amp;quot;var&amp;quot;) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_pca_df_km %&amp;gt;% plot_ly(x=~PC1, y=~PC2, z=~PC3, type=&amp;quot;scatter3d&amp;quot;, mode=&amp;quot;markers&amp;quot;, color = ~cluster) %&amp;gt;% layout(scene = list(xaxis = list(title = &amp;#39;PC1 (0.44)&amp;#39;), yaxis = list(title = &amp;#39;PC2 (0.24)&amp;#39;), zaxis = list(title = &amp;#39;PC3 (0.18)&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;68e822201d46&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;68e822201d46&#34;,&#34;attrs&#34;:{&#34;68e822201d46&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;z&#34;:{},&#34;mode&#34;:&#34;markers&#34;,&#34;color&#34;:{},&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;scatter3d&#34;}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:&#34;PC1 (0.44)&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;PC2 (0.24)&#34;},&#34;zaxis&#34;:{&#34;title&#34;:&#34;PC3 (0.18)&#34;}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:true},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;x&#34;:[-2.29319772221018,-2.11131140307473,-0.528766334892917,-2.33468913286766,-1.03793615459785,-2.08356305271039,-0.350059644500599,-0.818694302541133,-2.87978600311518,-0.5486819703151,-1.84031336754883,-2.51086753433792,-0.77240247364298,-2.44464391177211,-2.48042648733445,-0.184957946854164,-2.66923993620963,-2.03618441140133,-1.80348478273613],&#34;y&#34;:[0.592759393789722,-1.29076477750668,-1.62733269142585,-1.36167153236652,-0.598168804619145,-1.52225493577275,-0.871574691313242,0.295456128203461,2.256782135688,0.493399345359134,-0.0217521523986969,0.915242873299899,-2.22836665316224,-0.375858926629271,0.346329375583748,-0.373052886393018,0.932670344671229,0.414596230007585,-0.941809748419655],&#34;z&#34;:[0.256445374239515,0.866597831676071,2.35927179463706,0.280545018368283,1.8314226947022,-0.433248944726638,-0.655316679849994,0.128354444299685,-0.625537096908529,0.687434704339677,-0.304095670007428,-1.15961820470845,0.162891600506687,0.722945673710484,0.713693792291879,1.09151752134985,-0.787533889935342,0.1869624137359,0.077654794043675],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;1&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[2.94216287467319,1.94743472870561,-0.47822233006605,-0.664721550664497,1.13682464550325,0.822306348064632,1.07190906297059,-0.147141037458331,0.199294321369512,1.88563997508177,2.66113290298741,-0.110640822935078,0.514633853192868,0.713532673229052,2.77536371638002,0.844917543663161,1.37152976013352,0.52640377274337,0.744821107412191,0.334619938495907,0.267361548571376,2.02770365941909,1.22504178768941,0.552040137818934,1.2269703929894,0.944054171291118,2.1436573366832,1.62545414928106,2.62393333417113,0.231128778424715,-0.229940207158274],&#34;y&#34;:[-0.0752557943122514,1.05577150697445,-0.412077651900541,1.44721664016657,0.22413553987748,0.445263093303644,0.187234431605288,0.755372173672134,-0.023831098779958,2.73995454203208,1.15384861898201,-0.549686882245433,-0.576278812636121,0.69547339300874,1.30703210104687,0.261123615247009,-1.80817244966095,-0.493796508091662,-0.134875414259115,-0.0326198407507055,0.952726130821671,0.350305139464718,0.485101333969798,0.637158564961516,0.102537708857827,-0.793837447599091,0.613716038538981,-1.72861031213258,-4.18092893143258,0.00443657492663399,2.35693596974787],&#34;z&#34;:[0.666655165781602,0.191884751196139,-1.61515979102661,-1.07851228349137,0.798259695892016,-0.880936600577315,-0.0176822707947886,0.660594401635903,-0.501516275747051,-0.375985124631002,-0.458193452198609,-1.81410104491278,-1.0688829549961,0.455617103016485,0.263255998415282,1.41760066037626,-1.75352538943136,-0.99390360205101,-0.933292390651044,-0.144887248941461,1.14931647247675,0.019459868729898,0.252609340024118,-1.60339094145845,0.670218397936242,-1.24707169997073,0.979075178367292,2.9202965961821,-1.274701741879,-0.97388216592004,0.89039417688404],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;2&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;These are the same plots as previously shown; however, marker colors were changed to represent clusters defined by Kmeans. Clusters are somewhat prominent; Cluster 1 is high on PC1, Cluster 2 is low on PC1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-validate-optimal-k-for-pam&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5c. Validate Optimal K for PAM&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max_k = 10
pam_dat&amp;lt;-data %&amp;gt;% select_if(is.numeric) %&amp;gt;% scale
sil_width&amp;lt;-vector()
for(i in 2:max_k){  
  pam_fit &amp;lt;- pam(pam_dat, k = i)  
  sil_width[i] &amp;lt;- pam_fit$silinfo$avg.width  
}
ggplot()+geom_line(aes(x=1:max_k,y=sil_width))+scale_x_continuous(name=&amp;quot;k&amp;quot;,breaks=1:max_k) + xlim(2,max_k) + xlab(&amp;quot;K&amp;quot;) + ylab(&amp;quot;Mean Silhouette Width&amp;quot;) + theme_minimal() + ggtitle(&amp;quot;Mean Silhouette Width by K&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale for &amp;#39;x&amp;#39; is already present. Adding another scale for &amp;#39;x&amp;#39;, which will
## replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though k=6 has a higher silhouette score than k=4, I choose to set k=4. This is one of those interpretability-tightness tradeoffs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-assess-pam-cluster-goodness-with-another-mesa-themed-silhouette-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5c. Assess PAM Cluster “Goodness” with another (Mesa-themed) Silhouette Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gower1 &amp;lt;-daisy(data %&amp;gt;% select(-X1) %&amp;gt;% mutate_if(is.character,as.factor) %&amp;gt;% select_if(is.numeric) %&amp;gt;% scale,metric=&amp;quot;gower&amp;quot;) 
pam_fit &amp;lt;- pam(gower1,k = 4,diss =T)

plot(silhouette(pam_fit$clustering,gower1), col = c(&amp;#39;#E53D57&amp;#39;,&amp;quot;#D9563A&amp;quot;, &amp;quot;#E68E0C&amp;quot;, &amp;quot;#89669D&amp;quot;), border = NA,main =&amp;quot;Silhouette Plot of PAM Clusters&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s clear that with PAM, my data clusters a little more cleanly. The clusters still aren’t great (as seen by the very small 0.26 mean silhouette width). Cluster two is fairly tight, and cluster four is somewhat diffuse. Each cluster has one or two samples that don’t belong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-visualizing-kmeans-clusters-in-feature-space-with-principal-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5c. Visualizing Kmeans Clusters In Feature Space (with Principal Components)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_pca_df_pam &amp;lt;- data_pca_df %&amp;gt;% mutate(cluster=as.factor(pam_fit$clustering))
fviz_pca_biplot(data_pca, habillage = data_pca_df_pam$cluster, label =&amp;quot;var&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Project1/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_pca_df_pam %&amp;gt;% plot_ly(x=~PC1, y=~PC2, z=~PC3, type=&amp;quot;scatter3d&amp;quot;, mode=&amp;quot;markers&amp;quot;, color = ~cluster) %&amp;gt;% layout(scene = list(xaxis = list(title = &amp;#39;PC1 (0.44)&amp;#39;), yaxis = list(title = &amp;#39;PC2 (0.24)&amp;#39;), zaxis = list(title = &amp;#39;PC3 (0.18)&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;68e838c8e767&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;68e838c8e767&#34;,&#34;attrs&#34;:{&#34;68e838c8e767&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;z&#34;:{},&#34;mode&#34;:&#34;markers&#34;,&#34;color&#34;:{},&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;scatter3d&#34;}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:&#34;PC1 (0.44)&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;PC2 (0.24)&#34;},&#34;zaxis&#34;:{&#34;title&#34;:&#34;PC3 (0.18)&#34;}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:true},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;x&#34;:[-2.29319772221018,-2.11131140307473,-2.33468913286766,-2.08356305271039,-2.87978600311518,-1.84031336754883,-2.51086753433792,-0.77240247364298,-2.44464391177211,-2.48042648733445,-2.66923993620963,-2.03618441140133,-1.80348478273613],&#34;y&#34;:[0.592759393789722,-1.29076477750668,-1.36167153236652,-1.52225493577275,2.256782135688,-0.0217521523986969,0.915242873299899,-2.22836665316224,-0.375858926629271,0.346329375583748,0.932670344671229,0.414596230007585,-0.941809748419655],&#34;z&#34;:[0.256445374239515,0.866597831676071,0.280545018368283,-0.433248944726638,-0.625537096908529,-0.304095670007428,-1.15961820470845,0.162891600506687,0.722945673710484,0.713693792291879,-0.787533889935342,0.1869624137359,0.077654794043675],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;1&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(102,194,165,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[2.94216287467319,1.94743472870561,1.13682464550325,1.88563997508177,2.66113290298741,2.77536371638002,2.02770365941909,1.2269703929894,2.1436573366832,-0.229940207158274],&#34;y&#34;:[-0.0752557943122514,1.05577150697445,0.22413553987748,2.73995454203208,1.15384861898201,1.30703210104687,0.350305139464718,0.102537708857827,0.613716038538981,2.35693596974787],&#34;z&#34;:[0.666655165781602,0.191884751196139,0.798259695892016,-0.375985124631002,-0.458193452198609,0.263255998415282,0.019459868729898,0.670218397936242,0.979075178367292,0.89039417688404],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;2&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(252,141,98,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[-0.528766334892917,-0.664721550664497,-1.03793615459785,-0.147141037458331,-0.818694302541133,-0.5486819703151,0.713532673229052,0.844917543663161,0.267361548571376,-0.184957946854164],&#34;y&#34;:[-1.62733269142585,1.44721664016657,-0.598168804619145,0.755372173672134,0.295456128203461,0.493399345359134,0.69547339300874,0.261123615247009,0.952726130821671,-0.373052886393018],&#34;z&#34;:[2.35927179463706,-1.07851228349137,1.8314226947022,0.660594401635903,0.128354444299685,0.687434704339677,0.455617103016485,1.41760066037626,1.14931647247675,1.09151752134985],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;3&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(141,160,203,1)&#34;},&#34;frame&#34;:null},{&#34;x&#34;:[-0.47822233006605,0.822306348064632,-0.350059644500599,1.07190906297059,0.199294321369512,-0.110640822935078,0.514633853192868,1.37152976013352,0.52640377274337,0.744821107412191,0.334619938495907,1.22504178768941,0.552040137818934,0.944054171291118,1.62545414928106,2.62393333417113,0.231128778424715],&#34;y&#34;:[-0.412077651900541,0.445263093303644,-0.871574691313242,0.187234431605288,-0.023831098779958,-0.549686882245433,-0.576278812636121,-1.80817244966095,-0.493796508091662,-0.134875414259115,-0.0326198407507055,0.485101333969798,0.637158564961516,-0.793837447599091,-1.72861031213258,-4.18092893143258,0.00443657492663399],&#34;z&#34;:[-1.61515979102661,-0.880936600577315,-0.655316679849994,-0.0176822707947886,-0.501516275747051,-1.81410104491278,-1.0688829549961,-1.75352538943136,-0.99390360205101,-0.933292390651044,-0.144887248941461,0.252609340024118,-1.60339094145845,-1.24707169997073,2.9202965961821,-1.274701741879,-0.97388216592004],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter3d&#34;,&#34;name&#34;:&#34;4&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;}},&#34;textfont&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;line&#34;:{&#34;color&#34;:&#34;rgba(231,138,195,1)&#34;},&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;These principal components are the same as in the previous plot. The only element changed is the marker color, which now represents clusters determined via PAM.&lt;/p&gt;
&lt;p&gt;Cluster 1 is very low on PC1 and nearly 0 on PC2. Cluster 2 is very high on PC1 and almost 0 on PC2. It is impossible to differentiate clusters 3 and 4 using only two principal components. These each score near 0 on PC1 and PC2, but Cluster 3 scores slightly positive on PC3 while Cluster 4 scores slightly negative on PC3.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
